{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "routes = glob('/data/ephemeral/home/datas/text/*')\n",
    "data_route = 'datas/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting...: 100%|██████████| 100/100 [00:01<00:00, 60.84it/s]\n",
      "collecting...: 100%|██████████| 54/54 [00:00<00:00, 57.69it/s]\n",
      "collecting...: 100%|██████████| 100/100 [00:01<00:00, 72.60it/s]\n",
      "collecting...: 100%|██████████| 100/100 [00:01<00:00, 59.10it/s]\n",
      "collecting...: 100%|██████████| 100/100 [00:01<00:00, 66.28it/s]\n",
      "collecting...: 100%|██████████| 100/100 [00:01<00:00, 69.04it/s]\n",
      "collecting...: 100%|██████████| 100/100 [00:01<00:00, 61.64it/s]\n",
      "collecting...: 100%|██████████| 100/100 [00:01<00:00, 71.53it/s]\n",
      "collecting...: 100%|██████████| 100/100 [00:01<00:00, 53.80it/s]\n",
      "collecting...: 100%|██████████| 100/100 [00:01<00:00, 61.69it/s]\n",
      "collecting...: 100%|██████████| 100/100 [00:01<00:00, 74.10it/s]\n",
      "collecting...: 100%|██████████| 100/100 [00:01<00:00, 59.71it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "def parse_wiki_file(file_path):\n",
    "    documents = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # 문서별로 분리 (각 문서는 <doc>로 시작하고 </doc>로 끝남)\n",
    "    doc_pattern = re.compile(r'<doc id=\"(.*?)\" url=\"(.*?)\" title=\"(.*?)\">(.*?)</doc>', re.DOTALL)\n",
    "    matches = doc_pattern.findall(content)\n",
    "    \n",
    "    for match in matches:\n",
    "        doc_id, url, title, text = match\n",
    "        documents.append({\n",
    "            \"id\": doc_id,\n",
    "            \"url\": url,\n",
    "            \"title\": title,\n",
    "            \"text\": text.strip()\n",
    "        })\n",
    "    \n",
    "    return documents\n",
    "result = []\n",
    "for route in routes:\n",
    "    for r in tqdm(glob(route + '/*'), desc = 'collecting...'):\n",
    "        result.extend(parse_wiki_file(r))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(result)\n",
    "df['doc_len'] = df['text'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['doc_len'] > 100) & (df['doc_len'] < 2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def text_split(doc):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    chunks = text_splitter.split_text(doc)\n",
    "    return chunks\n",
    "\n",
    "df['splitted_text'] = df['text'].apply(text_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205343    [북극 세계기록보관소, 북극 세계기록보관소(Arctic World Archive, ...\n",
       "827610    [청주 문의문산관, 청주 문의문산관(淸州 文義文山館)은 충청북도 청주시 상당구 문의...\n",
       "775180    [이지안 (미스코리아)\\n\\n이지안(본명: 이은희, 1977년 7월 24일~)는 대...\n",
       "Name: splitted_text, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['splitted_text'].sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datas/test+keyword.csv'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_route.split('.csv')[0]+'+' +'keyword.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 767/869 [37:57<05:05,  3.00s/it]"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from kiwipiepy import Kiwi\n",
    "from keybert import KeyBERT\n",
    "from transformers import pipeline\n",
    "from ast import literal_eval\n",
    "from tqdm import tqdm\n",
    "\n",
    "kw_model = KeyBERT(model='sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')\n",
    "\n",
    "# 텍스트에서 주요 키워드 추출 함수 (KeyBERT 사용)\n",
    "def extract_keywords(doc, top_n=5, ngram = 2):\n",
    "    kiwi = Kiwi()\n",
    "    # 문장을 형태소 분석하여 명사, 형용사, 동사 등을 추출\n",
    "    tokens = kiwi.tokenize(doc)\n",
    "    words = [token.form for token in tokens if token.tag.startswith(('NN', 'VV', 'VA'))]  # 명사(NN), 동사(VV), 형용사(VA)만 추출\n",
    "\n",
    "    # KeyBERT 모델을 사용하여 주요 키워드 추출\n",
    "    keywords = kw_model.extract_keywords(' '.join(words), keyphrase_ngram_range=(1, ngram), stop_words=None, top_n=top_n)\n",
    "    \n",
    "    # 결과에서 키워드만 추출하여 리스트로 반환\n",
    "    return [keyword[0] for keyword in keywords]\n",
    "\n",
    "data = pd.read_csv(data_route)\n",
    "data['dic'] = data['problems'].apply(lambda x: literal_eval(x))\n",
    "data['question'] = data['dic'].apply(lambda x: x['question'])\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# data['keywords'] = data.progress_apply(lambda x: extract_keywords(x['paragraph'] + ' ' + x['question']), axis=1)\n",
    "data['keywords_1'] = data.progress_apply(lambda x: extract_keywords(x['paragraph'] + ' ' + x['question'], ngram = 1), axis=1)\n",
    "# 위 코드는 ngram 2개까지 아래코드는 ngram 1개만\n",
    "\n",
    "file_name = data_route.split('.csv')[0]+'+' +'keyword.csv'\n",
    "data[['id', 'paragraph', 'problems', 'question_plus', 'keywords', 'keywords_1']].to_csv(file_name, index = False)\n",
    "data = pd.read_csv(file_name)\n",
    "\n",
    "docs = []\n",
    "titles_set = set(df['title'].values)  \n",
    "cnt = 0\n",
    "\n",
    "for words in tqdm(data['keywords_1'].values):\n",
    "    for word in words:\n",
    "        if word in titles_set:\n",
    "            docs.append(df[df['title'] == word].to_dict('records'))\n",
    "\n",
    "# 위키 문서에서 title이 매칭되는 애들만 뽑기\n",
    "\n",
    "flattened_docs = [item for sublist in docs for item in sublist]\n",
    "\n",
    "df = pd.DataFrame(flattened_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "469578it [00:20, 23134.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "725969\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "for i ,item in tqdm(df.iterrows()):\n",
    "    corpus += item['splitted_text']\n",
    "corpus = list(filter(lambda x: len(x) > 100, corpus))\n",
    "print(len(corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_155496/453807431.py:15: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  ko_embedding = HuggingFaceEmbeddings(\n",
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making bm25..\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import Document\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
    "ko_embedding = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "docs = [Document(page_content=text, metadata={\"source\": f\"doc_{i}\"}) for i, text in enumerate(corpus)]\n",
    "\n",
    "print('making bm25..')\n",
    "# initialize the bm25 retriever and faiss retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "bm25_retriever.k = 2\n",
    "print('making dense retrieval..')\n",
    "faiss_vectorstore = FAISS.from_documents(docs, ko_embedding)\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]\n",
    ")   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "train = pd.read_csv('datas/test.csv')\n",
    "train['dic'] = train['problems'].apply(lambda x: literal_eval(x))\n",
    "train['question'] = train['dic'].apply(lambda x: x['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "동물 병원 원장 A( 3 5세 )는 차량의 소유자로부터 공작물인   차량을 장기간 임차하여 사용하고 있다 .  A는 고객 갑이 치료를  위해 입원시킨 반려견의 재활 운동을 위해 그 차량을 운전하여  직원 B( 2 1세 )와 함께 공원으로 갔다 .  A는 차량을 주차하고 B 와  함께 반려견을 데리고 산책하던 중 B가 부주의로 반려견의 발을  밟아 상처를 입혔다 .  놀란 반려견이 지나가던 을의 다리를 물어  2주간의 치료를 요하는 상처를 입혔고,  을은 이로 인해 정신적 으로도 큰 충격을 받았다 .  그 사이 A가 주차해 두었던 차량에서  불이 났고,  이로 인해 옆에 주차되어 있던 병 소유 차량이 파손 되는 재산상 손해가 발생하였다 .  사고 조사 결과,  화재의 원인은  차량에 대한 비전문가인 소유자가 해당 차량을 직접 수리하여   발생한 보존상의 하자에 의한 것으로 밝혀졌다 .\n",
      "다음 사례에 대한 법적 판단으로 옳은 것은 ?\n",
      "[Document(metadata={'source': 'doc_415645'}, page_content='(7) WRITE-ITEM C .\\n (8) READ-ITEM A ; IF END OF DATA GO TO OPERATION 14 .\\n (9) JUMP TO OPERATION 1 .\\n(10) READ-ITEM B ; IF END OF DATA GO TO OPERATION 12 .\\n(11) JUMP TO OPERATION 1 .\\n(12) SET OPERATION 9 TO GO TO OPERATION 2 .\\n(13) JUMP TO OPERATION 2 .\\n(14) TEST PRODUCT-NO (B) AGAINST ZZZZZZZZZZZZ ; IF EQUAL GO TO OPERATION 16 ;\\n OTHERWISE GO TO OPERATION 15 .\\n(15) REWIND B .\\n(16) CLOSE-OUT FILES C ; D .\\n이 샘플에는 오직 프로그램의 실행문들 섹션만 포함되어 있다. 레코드 필드 와 는 섹션에 정의되며, 여기에는 영어와 같은 문법을 이용하지는 않았다.'), Document(metadata={'source': 'doc_97819'}, page_content='\\\\new Staff \\\\relative c\\' { c1_\\\\markup{\"Cm\"}^\\\\markup { \\\\hspace #1 \\\\with-dimensions #\\'(0 . 0) #\\'(0 . 0) \\\\translate #\\'(0 . -5) \\\\draw-line #\\'(0 . 8) } d^\\\\markup { \\\\hspace #1 \\\\with-dimensions #\\'(0 . 0) #\\'(0 . 0) \\\\translate #\\'(0 . -4.5) \\\\draw-line #\\'(0 . 8) } es f^\\\\markup { \\\\hspace #1 \\\\with-dimensions #\\'(0 . 0) #\\'(0 . 0) \\\\translate #\\'(0 . -3.5) \\\\draw-line #\\'(0 . 8) } g^\\\\markup { \\\\hspace #1 \\\\with-dimensions #\\'(0 . 0) #\\'(0 . 0) \\\\translate #\\'(0 . -3) \\\\draw-line #\\'(0 . 8) } aes bes c^\\\\markup { \\\\hspace #1')]\n"
     ]
    }
   ],
   "source": [
    "idx = 42\n",
    "result = ensemble_retriever.get_relevant_documents(train['paragraph'][idx] + train['question'][idx])\n",
    "print(train['paragraph'][idx])\n",
    "print(train['question'][idx])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2031/2031 [13:45<00:00,  2.46it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def generate_hint(row):\n",
    "    query = row['question'] + ' ' + row['question']\n",
    "    relevant_docs = ensemble_retriever.get_relevant_documents(query)\n",
    "    \n",
    "    # `hint` 문자열 생성\n",
    "    hint = ''\n",
    "    for i, relevant_doc in enumerate(relevant_docs):\n",
    "        hint += f\"참고문서{i} : {relevant_doc.page_content}\\n\"\n",
    "        if i == 2:\n",
    "            break  # 3개까지만\n",
    "    return hint\n",
    "\n",
    "# tqdm 적용 및 `apply`를 사용하여 새 열 생성\n",
    "tqdm.pandas()\n",
    "train['hint'] = train.progress_apply(generate_hint, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('datas/train+klue.csv')\n",
    "d['hint'] = train['hint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "참고문서0 : 이 때에 자기의 의로우심을 나타내사 자기도 의로우시며 또한 예수 믿는 자를 의롭다 하려 하심이니라 그런즉 자랑할 데가 어디뇨 있을 수가 없느니라 무슨 법으로냐 행위로냐 아니라 오직 믿음의 법으로니라 그러므로 사람이 의롭다 하심을 얻는 것은 율법의 행위에 있지 않고 믿음으로 되는줄 우리가 인정하노라”\n",
      "참고문서1 : 이렇게 사찰의 건축물로서 지어졌던 탑도 있지만 민간에서 단순하게 주변에 있던 돌을 쌓아 올려 만든 돌탑도 있다. 소박하게 만들어진 이런 돌탑들은 토속신앙과 관련이 깊어 서낭신을 모시는 서낭당과 가까이 있는 경우가 많았으며, 기복의 용도로 이용되었다. 한국의 불탑은 중국과 일본에 비해 그 규모가 작은 편이다. 이는 오랜 전란으로 인한 문화재의 훼손이 잦았고, 조선의 숭유억불 정책으로 인해 의도적으로 사찰을 파괴하는 경우가 많았기 때문이다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(d['hint'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[['id', 'paragraph', 'problems', 'question_plus', 'klue', 'hint']].to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
