{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "routes = glob('/data/ephemeral/home/datas/text/*')\n",
    "data_route = 'datas/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting...: 100%|██████████| 100/100 [00:01<00:00, 60.84it/s]\n",
      "collecting...: 100%|██████████| 54/54 [00:00<00:00, 57.69it/s]\n",
      "collecting...: 100%|██████████| 100/100 [00:01<00:00, 72.60it/s]\n",
      "collecting...: 100%|██████████| 100/100 [00:01<00:00, 59.10it/s]\n",
      "collecting...: 100%|██████████| 100/100 [00:01<00:00, 66.28it/s]\n",
      "collecting...: 100%|██████████| 100/100 [00:01<00:00, 69.04it/s]\n",
      "collecting...: 100%|██████████| 100/100 [00:01<00:00, 61.64it/s]\n",
      "collecting...: 100%|██████████| 100/100 [00:01<00:00, 71.53it/s]\n",
      "collecting...: 100%|██████████| 100/100 [00:01<00:00, 53.80it/s]\n",
      "collecting...: 100%|██████████| 100/100 [00:01<00:00, 61.69it/s]\n",
      "collecting...: 100%|██████████| 100/100 [00:01<00:00, 74.10it/s]\n",
      "collecting...: 100%|██████████| 100/100 [00:01<00:00, 59.71it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "def parse_wiki_file(file_path):\n",
    "    documents = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # 문서별로 분리 (각 문서는 <doc>로 시작하고 </doc>로 끝남)\n",
    "    doc_pattern = re.compile(r'<doc id=\"(.*?)\" url=\"(.*?)\" title=\"(.*?)\">(.*?)</doc>', re.DOTALL)\n",
    "    matches = doc_pattern.findall(content)\n",
    "    \n",
    "    for match in matches:\n",
    "        doc_id, url, title, text = match\n",
    "        documents.append({\n",
    "            \"id\": doc_id,\n",
    "            \"url\": url,\n",
    "            \"title\": title,\n",
    "            \"text\": text.strip()\n",
    "        })\n",
    "    \n",
    "    return documents\n",
    "result = []\n",
    "for route in routes:\n",
    "    for r in tqdm(glob(route + '/*'), desc = 'collecting...'):\n",
    "        result.extend(parse_wiki_file(r))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(result)\n",
    "df['doc_len'] = df['text'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['doc_len'] > 100) & (df['doc_len'] < 2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def text_split(doc):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    chunks = text_splitter.split_text(doc)\n",
    "    return chunks\n",
    "\n",
    "df['splitted_text'] = df['text'].apply(text_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205343    [북극 세계기록보관소, 북극 세계기록보관소(Arctic World Archive, ...\n",
       "827610    [청주 문의문산관, 청주 문의문산관(淸州 文義文山館)은 충청북도 청주시 상당구 문의...\n",
       "775180    [이지안 (미스코리아)\\n\\n이지안(본명: 이은희, 1977년 7월 24일~)는 대...\n",
       "Name: splitted_text, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['splitted_text'].sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datas/test+keyword.csv'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_route.split('.csv')[0]+'+' +'keyword.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 869/869 [02:20<00:00,  6.18it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from kiwipiepy import Kiwi\n",
    "from keybert import KeyBERT\n",
    "from transformers import pipeline\n",
    "from ast import literal_eval\n",
    "from tqdm import tqdm\n",
    "\n",
    "kw_model = KeyBERT(model='sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')\n",
    "\n",
    "# 텍스트에서 주요 키워드 추출 함수 (KeyBERT 사용)\n",
    "def extract_keywords(doc, top_n=5, ngram = 2):\n",
    "    kiwi = Kiwi()\n",
    "    # 문장을 형태소 분석하여 명사, 형용사, 동사 등을 추출\n",
    "    tokens = kiwi.tokenize(doc)\n",
    "    words = [token.form for token in tokens if token.tag.startswith(('NN', 'VV', 'VA'))]  # 명사(NN), 동사(VV), 형용사(VA)만 추출\n",
    "\n",
    "    # KeyBERT 모델을 사용하여 주요 키워드 추출\n",
    "    keywords = kw_model.extract_keywords(' '.join(words), keyphrase_ngram_range=(1, ngram), stop_words=None, top_n=top_n)\n",
    "    \n",
    "    # 결과에서 키워드만 추출하여 리스트로 반환\n",
    "    return [keyword[0] for keyword in keywords]\n",
    "\n",
    "data = pd.read_csv(data_route)\n",
    "data['dic'] = data['problems'].apply(lambda x: literal_eval(x))\n",
    "data['question'] = data['dic'].apply(lambda x: x['question'])\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# data['keywords'] = data.progress_apply(lambda x: extract_keywords(x['paragraph'] + ' ' + x['question']), axis=1)\n",
    "data['keywords_1'] = data.progress_apply(lambda x: extract_keywords(x['paragraph'] + ' ' + x['question'], ngram = 1), axis=1)\n",
    "# 위 코드는 ngram 2개까지 아래코드는 ngram 1개만\n",
    "\n",
    "file_name = data_route.split('.csv')[0]+'+' +'keyword.csv'\n",
    "data[['id', 'paragraph', 'problems', 'question_plus', 'keywords_1']].to_csv(file_name, index = False)\n",
    "data = pd.read_csv(file_name)\n",
    "\n",
    "docs = []\n",
    "titles_set = set(df['title'].values)  \n",
    "cnt = 0\n",
    "\n",
    "for words in tqdm(data['keywords_1'].values):\n",
    "    for word in words:\n",
    "        if word in titles_set:\n",
    "            docs.append(df[df['title'] == word].to_dict('records'))\n",
    "\n",
    "# 위키 문서에서 title이 매칭되는 애들만 뽑기\n",
    "\n",
    "flattened_docs = [item for sublist in docs for item in sublist]\n",
    "\n",
    "df = pd.DataFrame(flattened_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2119it [00:00, 22013.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "for i ,item in tqdm(df.iterrows()):\n",
    "    corpus += item['splitted_text']\n",
    "corpus = list(filter(lambda x: len(x) > 100, corpus))\n",
    "print(len(corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_155974/3318276372.py:15: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  ko_embedding = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making bm25..\n",
      "making dense retrieval..\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import Document\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
    "ko_embedding = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "docs = [Document(page_content=text, metadata={\"source\": f\"doc_{i}\"}) for i, text in enumerate(corpus)]\n",
    "\n",
    "print('making bm25..')\n",
    "# initialize the bm25 retriever and faiss retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "bm25_retriever.k = 2\n",
    "print('making dense retrieval..')\n",
    "faiss_vectorstore = FAISS.from_documents(docs, ko_embedding)\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]\n",
    ")   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "data = pd.read_csv('datas/test.csv')\n",
    "data['dic'] = data['problems'].apply(lambda x: literal_eval(x))\n",
    "data['question'] = data['dic'].apply(lambda x: x['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_155974/1343393623.py:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = ensemble_retriever.get_relevant_documents(data['paragraph'][idx] + data['question'][idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "동물 병원 원장 A( 3 5세 )는 차량의 소유자로부터 공작물인   차량을 장기간 임차하여 사용하고 있다 .  A는 고객 갑이 치료를  위해 입원시킨 반려견의 재활 운동을 위해 그 차량을 운전하여  직원 B( 2 1세 )와 함께 공원으로 갔다 .  A는 차량을 주차하고 B 와  함께 반려견을 데리고 산책하던 중 B가 부주의로 반려견의 발을  밟아 상처를 입혔다 .  놀란 반려견이 지나가던 을의 다리를 물어  2주간의 치료를 요하는 상처를 입혔고,  을은 이로 인해 정신적 으로도 큰 충격을 받았다 .  그 사이 A가 주차해 두었던 차량에서  불이 났고,  이로 인해 옆에 주차되어 있던 병 소유 차량이 파손 되는 재산상 손해가 발생하였다 .  사고 조사 결과,  화재의 원인은  차량에 대한 비전문가인 소유자가 해당 차량을 직접 수리하여   발생한 보존상의 하자에 의한 것으로 밝혀졌다 .\n",
      "다음 사례에 대한 법적 판단으로 옳은 것은 ?\n",
      "[Document(metadata={'source': 'doc_2995'}, page_content='인간이 발명한 최초의 스포츠 장비는 공이었다. 고대 이집트에서는 돌 던지기가 아이들이 가장 좋아하는 놀이였지만, 잘못 던진 돌은 아이에게 상처를 입힐 수 있었다. 그래서 이집트인들은 던지기에 덜 위험한 것을 찾고 있었다. 그리고 그들은 아마도 최초의 공이었던 것을 개발했다. 그것들은 처음에 실로 함께 묶은 풀이나 나뭇잎으로 만들어졌고, 나중에는 바느질해 붙이고 속에 깃털이나 짚으로 채워 넣은 동물 가죽 조각으로 만들어졌다. 시민들이 스포츠에 직접 참가하는 일이 드물었던 고대 로마에서도 공놀이는 가장 인기 높은 스포츠였다.'), Document(metadata={'source': 'doc_1456'}, page_content='학설.\\n여러 견해가 있으며 어느 하나의 견해에만 입각하여 죄수를 결정할 수 없다. 죄란 구성 요건을 전제로 한 개념이므로, 구성 요건 표준설을 우선적 기준으로 삼되 행위의 개수 범죄 의사 및 법익 등을 종합적으로 고려하여 각각의 범죄에 합당한 기준을 찾아야 한다.\\n각주.\\n&lt;templatestyles src=\"각주/styles.css\" /&gt;')]\n"
     ]
    }
   ],
   "source": [
    "idx = 42\n",
    "result = ensemble_retriever.get_relevant_documents(data['paragraph'][idx] + data['question'][idx])\n",
    "print(data['paragraph'][idx])\n",
    "print(data['question'][idx])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 869/869 [00:55<00:00, 15.54it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def generate_hint(row):\n",
    "    query = row['question'] + ' ' + row['question']\n",
    "    relevant_docs = ensemble_retriever.get_relevant_documents(query)\n",
    "    \n",
    "    # `hint` 문자열 생성\n",
    "    hint = ''\n",
    "    for i, relevant_doc in enumerate(relevant_docs):\n",
    "        hint += f\"참고문서{i} : {relevant_doc.page_content}\\n\"\n",
    "        if i == 2:\n",
    "            break  # 3개까지만\n",
    "    return hint\n",
    "\n",
    "# tqdm 적용 및 `apply`를 사용하여 새 열 생성\n",
    "tqdm.pandas()\n",
    "data['hint'] = train.progress_apply(generate_hint, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv(data_route)\n",
    "d['hint'] = data['hint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "참고문서0 : 철학에서 다루는 신에 대한 것으로는 “신 없이도 도덕이 가능한가?”라는 것이있다. 일반적으로 신을 믿는 사람은 그 종교에서 요구하는 도덕적 명령을 따르거나, 따르려고 애쓴다. 따라서 예전부터 이러한 신과 종교가 도덕을 가능하게 한다는 주장이 있었다. 이에 대해 다양한 주장이 있으며 “신이 도덕을 명령하기 때문에 도덕을 따라야 하는가, 아니면 그 도덕이 선한 것이기 때문에 따라야 하는가?”와 같은 반박도 있다.\n",
      "신의 존재에 관한 생각.\n",
      "리처드 도킨스는 신의 존재 여부에 대해 인간의 생각을 대략적으로 다음과 같이 분류하였다.\n",
      "출처.\n",
      "&lt;templatestyles src=\"각주/styles.css\" /&gt;\n",
      "참고문서1 : 학설.\n",
      "여러 견해가 있으며 어느 하나의 견해에만 입각하여 죄수를 결정할 수 없다. 죄란 구성 요건을 전제로 한 개념이므로, 구성 요건 표준설을 우선적 기준으로 삼되 행위의 개수 범죄 의사 및 법익 등을 종합적으로 고려하여 각각의 범죄에 합당한 기준을 찾아야 한다.\n",
      "각주.\n",
      "&lt;templatestyles src=\"각주/styles.css\" /&gt;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(d['hint'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[['id', 'paragraph', 'problems', 'question_plus', 'hint']].to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
