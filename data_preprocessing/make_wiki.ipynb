{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "routes = glob('/data/ephemeral/home/datas/text/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting...:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting...: 100%|██████████| 100/100 [00:01<00:00, 60.95it/s]\n",
      "collecting...: 100%|██████████| 54/54 [00:00<00:00, 59.09it/s]\n",
      "collecting...: 100%|██████████| 100/100 [00:01<00:00, 72.21it/s]\n",
      "collecting...: 100%|██████████| 100/100 [00:01<00:00, 59.19it/s]\n",
      "collecting...: 100%|██████████| 100/100 [00:01<00:00, 65.57it/s]\n",
      "collecting...: 100%|██████████| 100/100 [00:01<00:00, 68.66it/s]\n",
      "collecting...: 100%|██████████| 100/100 [00:01<00:00, 61.41it/s]\n",
      "collecting...: 100%|██████████| 100/100 [00:01<00:00, 71.77it/s]\n",
      "collecting...: 100%|██████████| 100/100 [00:01<00:00, 54.28it/s]\n",
      "collecting...: 100%|██████████| 100/100 [00:01<00:00, 61.97it/s]\n",
      "collecting...: 100%|██████████| 100/100 [00:01<00:00, 74.92it/s]\n",
      "collecting...: 100%|██████████| 100/100 [00:01<00:00, 59.77it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "def parse_wiki_file(file_path):\n",
    "    documents = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # 문서별로 분리 (각 문서는 <doc>로 시작하고 </doc>로 끝남)\n",
    "    doc_pattern = re.compile(r'<doc id=\"(.*?)\" url=\"(.*?)\" title=\"(.*?)\">(.*?)</doc>', re.DOTALL)\n",
    "    matches = doc_pattern.findall(content)\n",
    "    \n",
    "    for match in matches:\n",
    "        doc_id, url, title, text = match\n",
    "        documents.append({\n",
    "            \"id\": doc_id,\n",
    "            \"url\": url,\n",
    "            \"title\": title,\n",
    "            \"text\": text.strip()\n",
    "        })\n",
    "    \n",
    "    return documents\n",
    "result = []\n",
    "for route in routes:\n",
    "    for r in tqdm(glob(route + '/*'), desc = 'collecting...'):\n",
    "        result.extend(parse_wiki_file(r))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(result)\n",
    "df['doc_len'] = df['text'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['doc_len'] > 100) & (df['doc_len'] < 10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def text_split(doc):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    chunks = text_splitter.split_text(doc)\n",
    "    return chunks\n",
    "\n",
    "df['splitted_text'] = df['text'].apply(text_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "606958     [안드레아스 쾨프케, 안드레아스 쾨프케(: Andreas Köpke, 1962년 3...\n",
       "821034     [우에다역 (나가노현)\\n\\n우에다역(: 上田駅, うえだえき)은 일본 나가노현 우에...\n",
       "1295171    [성교, 성교(性交) 또는 성관계(性關係, : sexual intercourse)는...\n",
       "Name: splitted_text, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['splitted_text'].sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>problems</th>\n",
       "      <th>question_plus</th>\n",
       "      <th>dic</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>generation-for-nlp-0</td>\n",
       "      <td>사람들이 지속적으로 책을 읽는 이유 중 하나는 즐거움이다 .   독서의 즐거움에는 ...</td>\n",
       "      <td>{'question': '윗글의 내용과 일치하지 않는  것은?', 'choices'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'question': '윗글의 내용과 일치하지 않는  것은?', 'choices'...</td>\n",
       "      <td>윗글의 내용과 일치하지 않는  것은?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>generation-for-nlp-1</td>\n",
       "      <td>사람들이 지속적으로 책을 읽는 이유 중 하나는 즐거움이다 .   독서의 즐거움에는 ...</td>\n",
       "      <td>{'question': '윗글을 읽고 ㉠에 대해 보인 반응으로 적절하지 않은  것은...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'question': '윗글을 읽고 ㉠에 대해 보인 반응으로 적절하지 않은  것은...</td>\n",
       "      <td>윗글을 읽고 ㉠에 대해 보인 반응으로 적절하지 않은  것은?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>generation-for-nlp-2</td>\n",
       "      <td>(가 ) 중국에서 비롯된 유서( 類書)는 고금의 서적에서 자료를  수집하고 항목별로...</td>\n",
       "      <td>{'question': '(가 )와 (나 )에 대한 설명으로 가장 적절한 것은?',...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'question': '(가 )와 (나 )에 대한 설명으로 가장 적절한 것은?',...</td>\n",
       "      <td>(가 )와 (나 )에 대한 설명으로 가장 적절한 것은?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                          paragraph  \\\n",
       "0  generation-for-nlp-0  사람들이 지속적으로 책을 읽는 이유 중 하나는 즐거움이다 .   독서의 즐거움에는 ...   \n",
       "1  generation-for-nlp-1  사람들이 지속적으로 책을 읽는 이유 중 하나는 즐거움이다 .   독서의 즐거움에는 ...   \n",
       "2  generation-for-nlp-2  (가 ) 중국에서 비롯된 유서( 類書)는 고금의 서적에서 자료를  수집하고 항목별로...   \n",
       "\n",
       "                                            problems question_plus  \\\n",
       "0  {'question': '윗글의 내용과 일치하지 않는  것은?', 'choices'...           NaN   \n",
       "1  {'question': '윗글을 읽고 ㉠에 대해 보인 반응으로 적절하지 않은  것은...           NaN   \n",
       "2  {'question': '(가 )와 (나 )에 대한 설명으로 가장 적절한 것은?',...           NaN   \n",
       "\n",
       "                                                 dic  \\\n",
       "0  {'question': '윗글의 내용과 일치하지 않는  것은?', 'choices'...   \n",
       "1  {'question': '윗글을 읽고 ㉠에 대해 보인 반응으로 적절하지 않은  것은...   \n",
       "2  {'question': '(가 )와 (나 )에 대한 설명으로 가장 적절한 것은?',...   \n",
       "\n",
       "                            question  \n",
       "0               윗글의 내용과 일치하지 않는  것은?  \n",
       "1  윗글을 읽고 ㉠에 대해 보인 반응으로 적절하지 않은  것은?  \n",
       "2     (가 )와 (나 )에 대한 설명으로 가장 적절한 것은?  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from kiwipiepy import Kiwi\n",
    "from keybert import KeyBERT\n",
    "from transformers import pipeline\n",
    "from ast import literal_eval\n",
    "from tqdm import tqdm\n",
    "\n",
    "kw_model = KeyBERT(model='sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')\n",
    "\n",
    "# 텍스트에서 주요 키워드 추출 함수 (KeyBERT 사용)\n",
    "def extract_keywords(doc, top_n=5, ngram = 2):\n",
    "    kiwi = Kiwi()\n",
    "    # 문장을 형태소 분석하여 명사, 형용사, 동사 등을 추출\n",
    "    tokens = kiwi.tokenize(doc)\n",
    "    words = [token.form for token in tokens if token.tag.startswith(('NN', 'VV', 'VA'))]  # 명사(NN), 동사(VV), 형용사(VA)만 추출\n",
    "\n",
    "    # KeyBERT 모델을 사용하여 주요 키워드 추출\n",
    "    keywords = kw_model.extract_keywords(' '.join(words), keyphrase_ngram_range=(1, ngram), stop_words=None, top_n=top_n)\n",
    "    \n",
    "    # 결과에서 키워드만 추출하여 리스트로 반환\n",
    "    return [keyword[0] for keyword in keywords]\n",
    "\n",
    "train = pd.read_csv('datas/test.csv')\n",
    "train['dic'] = train['problems'].apply(lambda x: literal_eval(x))\n",
    "train['question'] = train['dic'].apply(lambda x: x['question'])\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "train['keywords'] = train.progress_apply(lambda x: extract_keywords(x['paragraph'] + ' ' + x['question']), axis=1)\n",
    "train['keywords_1'] = train.progress_apply(lambda x: extract_keywords(x['paragraph'] + ' ' + x['question'], ngram = 1), axis=1)\n",
    "# 위 코드는 ngram 2개까지 아래코드는 ngram 1개만\n",
    "\n",
    "train[['id', 'paragraph', 'problems', 'question_plus', 'keywords', 'keywords_1']].to_csv('datas/train+keyword.csv', index = False)\n",
    "train = pd.read_csv('datas/train+keyword.csv')\n",
    "\n",
    "docs = []\n",
    "titles_set = set(df['title'].values)  \n",
    "cnt = 0\n",
    "\n",
    "for words in tqdm(train['keywords_1'].values):\n",
    "    for word in words:\n",
    "        if word in titles_set:\n",
    "            docs.append(df[df['title'] == word].to_dict('records'))\n",
    "\n",
    "# 위키 문서에서 title이 매칭되는 애들만 뽑기\n",
    "\n",
    "flattened_docs = [item for sublist in docs for item in sublist]\n",
    "\n",
    "df = pd.DataFrame(flattened_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "511081it [00:22, 22878.26it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "for i ,item in tqdm(df.iterrows()):\n",
    "    corpus += item['splitted_text']\n",
    "corpus = list(filter(lambda x: len(x) > 10, corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import Document\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
    "ko_embedding = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "docs = [Document(page_content=text, metadata={\"source\": f\"doc_{i}\"}) for i, text in enumerate(corpus)]\n",
    "\n",
    "# initialize the bm25 retriever and faiss retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "bm25_retriever.k = 2\n",
    "\n",
    "faiss_vectorstore = FAISS.from_documents(docs, ko_embedding)\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]\n",
    ")   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train['dic'] = train['problems'].apply(lambda x: literal_eval(x))\n",
    "train['question'] = train['dic'].apply(lambda x: x['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "선비들 수만 명이 대궐 앞에 모여 만 동묘와 서원을 다시 설립할 것을 청하니, (가)이/가 크게 노하여 한성부의 조례(皂隷)와 병졸로 하여 금 한 강 밖으로 몰아내게 하고 드디어 천여 곳의 서원을 철폐하고 그 토지를 몰수하여 관에 속하게 하였다.－대한계년사 －\n",
      "(가) 인물이 추진한 정책으로 옳지 않은 것은?\n",
      "[Document(metadata={'source': 'doc_25017'}, page_content='하고 그 위에 주거나 신전·궁전을 만든다. 이러한 일이 반복되어 수십 층으로 퇴적이 되는 것이다.'), Document(metadata={'source': 'doc_50729'}, page_content='영국해협에서 내륙으로 15km 들어간 지점의 오른 강 연안에 위치한다. 오른 강과 운하를 통해 영국 해협과 연결되는 항구도시이다. 11세기 정복왕 윌리엄(윌리엄 1세) 시대 때 노르망디의 한 중심지가 되었고, 그도 이 곳에 묻혔다. 1346년 에드워드 3세가 정복하여 잉글랜드의 지배하에 있다가 1450년 프랑스로 넘어왔다. 제2차 세계대전 중 노르망디 상륙 작전 때 이 도시는 중요한 요충지 중 한 곳이었으며, 크게 파괴되었다가 전쟁 후 근대적인 도시로 재건되었다. 11세기에 건립되어 윌리엄 왕과 왕비 마틸다가 각각 묻힌 생테티엔')]\n"
     ]
    }
   ],
   "source": [
    "idx = 4\n",
    "result = ensemble_retriever.get_relevant_documents(train['paragraph'][idx] + train['question'][idx])\n",
    "print(train['paragraph'][idx])\n",
    "print(train['question'][idx])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2031/2031 [13:45<00:00,  2.46it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def generate_hint(row):\n",
    "    query = row['question'] + ' ' + row['question']\n",
    "    relevant_docs = ensemble_retriever.get_relevant_documents(query)\n",
    "    \n",
    "    # `hint` 문자열 생성\n",
    "    hint = ''\n",
    "    for i, relevant_doc in enumerate(relevant_docs):\n",
    "        hint += f\"참고문서{i} : {relevant_doc.page_content}\\n\"\n",
    "        if i == 2:\n",
    "            break  # 3개까지만\n",
    "    return hint\n",
    "\n",
    "# tqdm 적용 및 `apply`를 사용하여 새 열 생성\n",
    "tqdm.pandas()\n",
    "train['hint'] = train.progress_apply(generate_hint, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('datas/train+klue.csv')\n",
    "d['hint'] = train['hint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "참고문서0 : 이 때에 자기의 의로우심을 나타내사 자기도 의로우시며 또한 예수 믿는 자를 의롭다 하려 하심이니라 그런즉 자랑할 데가 어디뇨 있을 수가 없느니라 무슨 법으로냐 행위로냐 아니라 오직 믿음의 법으로니라 그러므로 사람이 의롭다 하심을 얻는 것은 율법의 행위에 있지 않고 믿음으로 되는줄 우리가 인정하노라”\n",
      "참고문서1 : 이렇게 사찰의 건축물로서 지어졌던 탑도 있지만 민간에서 단순하게 주변에 있던 돌을 쌓아 올려 만든 돌탑도 있다. 소박하게 만들어진 이런 돌탑들은 토속신앙과 관련이 깊어 서낭신을 모시는 서낭당과 가까이 있는 경우가 많았으며, 기복의 용도로 이용되었다. 한국의 불탑은 중국과 일본에 비해 그 규모가 작은 편이다. 이는 오랜 전란으로 인한 문화재의 훼손이 잦았고, 조선의 숭유억불 정책으로 인해 의도적으로 사찰을 파괴하는 경우가 많았기 때문이다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(d['hint'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[['id', 'paragraph', 'problems', 'question_plus', 'klue', 'hint']].to_csv('train+klue+hint.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
