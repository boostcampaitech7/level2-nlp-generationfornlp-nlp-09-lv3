{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.49s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import transformers\n",
    "from ast import literal_eval\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM, SFTConfig\n",
    "\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import evaluate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq, DataCollatorForLanguageModeling\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# 1. 설정: pandas 출력 옵션 및 시드 고정\n",
    "pd.set_option('display.max_columns', None)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "set_seed(42)\n",
    "# 2. 모델 및 토크나이저 로드 (8-bit 양자화)\n",
    "model_name = \"CarrotAI/Llama-3.2-Rabbit-Ko-3B-Instruct\"\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_8bit=True,            # 8-bit 정밀도로 모델 로드\n",
    "#     llm_int8_threshold=6.0,       # int8 양자화 임계값\n",
    "#     llm_int8_has_fp16_weight=False # FP16 가중치 사용 여부\n",
    "# )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 3. PEFT 설정 (LoRA)\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=['q_proj', 'v_proj'],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "# model.gradient_checkpointing_enable()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'yongari_generate_91578', 'messages': [{'role': 'system', 'content': '대한민국 수능 전문가이다. 수능 국어 지문과 질문 그리고 선택지가 주어졌을 때 정답을 맞출 수 있다.'}, {'role': 'user', 'content': '\\n문제를 풀이할 때, 반드시 지문을 참고하세요.\\n문제는 무조건 1개의 정답만 있습니다.\\n문제를 풀이할 때 모든 선택지들을 검토하세요.\\n모든 선택지마다 근거를 지문에서 찾아 설명하세요.\\n\\n다음의 형식을 따라 답변하세요.\\n\\n1번: (선택지 1번에 대한 답변) + \"(지문 속 근거가 된 문장)\"\\n2번: (선택지 2번에 대한 답변) + \"(지문 속 근거가 된 문장)\"\\n3번: (선택지 3번에 대한 답변) + \"(지문 속 근거가 된 문장)\"\\n4번: (선택지 4번에 대한 답변) + \"(지문 속 근거가 된 문장)\"\\n5번: (선택지 5번에 대한 답변) + \"(지문 속 근거가 된 문장)\"\\n최종 정답: (근거 : 고른 정답에 대한 근거 # 정답 번호)\\n\\n지문 : \\n상소하여 아뢰기를 , “신이 좌참 찬 송준길이 올린 차자를 보았는데 , 상복(喪服) 절차에 대하여 논한 것이 신과는 큰 차이가 있었습니다 . 장자를 위하여 3년을 입는 까닭은 위로 ‘정체(正體)’가 되기 때문이고 또 전 중(傳重: 조상의 제사나 가문의 법통을 전함)하기 때문입니다 . …(중략) … 무엇보다 중요한 것은 할아버지와 아버지의 뒤를 이은 ‘정체’이지, 꼭 첫째이기 때문에 참 최 3년 복을 입는 것은 아닙니다 .”라고 하였다 .－현종실록 －ㄱ.기 사환국으로 정권을 장악하였다 .ㄴ.인 조반정을 주도 하여 집권세력이 되었다 .ㄷ.정조 시기에 탕평 정치의 한 축을 이루었다 .ㄹ.이 이와 성혼의 문인을 중심으로 형성되었다.\\n\\n질문 : \\n상소한 인물이 속한 붕당에 대한 설명으로 옳은 것만을 모두 고르면?\\n\\n선택지 : \\n1 - ㄱ, ㄴ\\n2 - ㄱ, ㄷ\\n3 - ㄴ, ㄹ\\n4 - ㄷ, ㄹ\\n\\n\\n1번, 2번, 3번, 4번, 5번 중에 하나를 정답으로 고르고, 근거와 고른 정답을 작성하세요.\\n근거와 정답은 \"#\"으로 구분하세요.\\n근거 :\\n'}, {'role': 'assistant', 'content': '근거 : 내용 상소한 인물은 송준길로, 그는 동인(서인과 반대되는 파)과 관련된 인물이다. 동인은 정조 시기에 탕평 정치의 한 축을 이루었고, 이 이와 성혼의 문인을 중심으로 형성된 것으로 알려져 있다. 따라서 기 사환국으로 정권을 장악한 사실은 동인과는 관련이 없다. # 2'}], 'label': '근거 : 내용 상소한 인물은 송준길로, 그는 동인(서인과 반대되는 파)과 관련된 인물이다. 동인은 정조 시기에 탕평 정치의 한 축을 이루었고, 이 이와 성혼의 문인을 중심으로 형성된 것으로 알려져 있다. 따라서 기 사환국으로 정권을 장악한 사실은 동인과는 관련이 없다. # 2'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'messages', 'label'],\n",
       "    num_rows: 2031\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "PROMPT_NO_QUESTION_PLUS = \"\"\"\n",
    "문제를 풀이할 때, 반드시 지문을 참고하세요.\n",
    "문제는 무조건 1개의 정답만 있습니다.\n",
    "문제를 풀이할 때 모든 선택지들을 검토하세요.\n",
    "모든 선택지마다 근거를 지문에서 찾아 설명하세요.\n",
    "\n",
    "다음의 형식을 따라 답변하세요.\n",
    "\n",
    "1번: (선택지 1번에 대한 답변) + \"(지문 속 근거가 된 문장)\"\n",
    "2번: (선택지 2번에 대한 답변) + \"(지문 속 근거가 된 문장)\"\n",
    "3번: (선택지 3번에 대한 답변) + \"(지문 속 근거가 된 문장)\"\n",
    "4번: (선택지 4번에 대한 답변) + \"(지문 속 근거가 된 문장)\"\n",
    "5번: (선택지 5번에 대한 답변) + \"(지문 속 근거가 된 문장)\"\n",
    "최종 정답: (근거 : 고른 정답에 대한 근거 # 정답 번호)\n",
    "\n",
    "지문 : \n",
    "{paragraph}\n",
    "\n",
    "질문 : \n",
    "{question}\n",
    "\n",
    "선택지 : \n",
    "{choices}\n",
    "\n",
    "\n",
    "1번, 2번, 3번, 4번, 5번 중에 하나를 정답으로 고르고, 근거와 고른 정답을 작성하세요.\n",
    "근거와 정답은 \"#\"으로 구분하세요.\n",
    "근거 :\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_QUESTION_PLUS = \"\"\"\n",
    "\n",
    "문제를 풀이할 때, 반드시 지문을 참고하세요.\n",
    "문제는 무조건 1개의 정답만 있습니다.\n",
    "문제를 풀이할 때 모든 선택지들을 검토하세요.\n",
    "모든 선택지마다 근거를 지문에서 찾아 설명하세요.\n",
    "\n",
    "다음의 형식을 따라 답변하세요.\n",
    "1번: (선택지 1번에 대한 답변) + \"(지문 속 근거가 된 문장)\"\n",
    "2번: (선택지 2번에 대한 답변) + \"(지문 속 근거가 된 문장)\"\n",
    "3번: (선택지 3번에 대한 답변) + \"(지문 속 근거가 된 문장)\"\n",
    "4번: (선택지 4번에 대한 답변) + \"(지문 속 근거가 된 문장)\"\n",
    "5번: (선택지 5번에 대한 답변) + \"(지문 속 근거가 된 문장)\"\n",
    "최종 정답: (근거 : 고른 정답에 대한 근거 # 정답 번호)\n",
    "\n",
    "지문 : \n",
    "{paragraph}\n",
    "질문 : \n",
    "{question}\n",
    "<보기> : \n",
    "{question_plus}\n",
    "선택지 : \n",
    "{choices}\n",
    "\n",
    "1번, 2번, 3번, 4번, 5번 중에 하나를 정답으로 고르고, 근거와 고른 정답을 작성하세요.\n",
    "근거와 정답은 \"#\"으로 구분하세요.\n",
    "근거 :\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "dataset = pd.read_csv('datas/train+klue.csv')\n",
    "# Load the train dataset\n",
    "# TODO Train Data 경로 입력\n",
    "\n",
    "# Flatten the JSON dataset\n",
    "records = []\n",
    "for _, row in dataset.iterrows():\n",
    "    problems = literal_eval(row['problems'])\n",
    "    record = {\n",
    "        'id': row['id'],\n",
    "        'paragraph': row['paragraph'],\n",
    "        'question': problems['question'],\n",
    "        'choices': problems['choices'],\n",
    "        'answer': problems.get('klue', None),\n",
    "        \"question_plus\": problems.get('question_plus', None),\n",
    "        'klue' : row.get('klue', None)\n",
    "    }\n",
    "    # Include 'question_plus' if it exists\n",
    "    if 'question_plus' in problems:\n",
    "        record['question_plus'] = problems['question_plus']\n",
    "    records.append(record)\n",
    "        \n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "dataset = Dataset.from_pandas(df)\n",
    "processed_dataset = []\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    choices_string = \"\\n\".join([f\"{idx + 1} - {choice}\" for idx, choice in enumerate(dataset[i][\"choices\"])])\n",
    "\n",
    "    # <보기>가 있을 때\n",
    "    if dataset[i][\"question_plus\"]:\n",
    "        user_message = PROMPT_QUESTION_PLUS.format(\n",
    "            paragraph=dataset[i][\"paragraph\"],\n",
    "            question=dataset[i][\"question\"],\n",
    "            question_plus=dataset[i][\"question_plus\"],\n",
    "            choices=choices_string,\n",
    "        )\n",
    "    # <보기>가 없을 때\n",
    "    else:\n",
    "        user_message = PROMPT_NO_QUESTION_PLUS.format(\n",
    "            paragraph=dataset[i][\"paragraph\"],\n",
    "            question=dataset[i][\"question\"],\n",
    "            choices=choices_string,\n",
    "        )\n",
    "\n",
    "    # chat message 형식으로 변환\n",
    "    processed_dataset.append(\n",
    "        {\n",
    "            \"id\": dataset[i][\"id\"],\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"\"\"대한민국 수능 전문가이다. 수능 국어 지문과 질문 그리고 선택지가 주어졌을 때 정답을 맞출 수 있다.\"\"\"},\n",
    "                {\"role\": \"user\", \"content\": user_message},\n",
    "                {\"role\": \"assistant\", \"content\": f\"{dataset[i]['klue']}\"}\n",
    "            ],\n",
    "            \"label\": dataset[i]['klue'],\n",
    "        }\n",
    "    )\n",
    "print(processed_dataset[0])\n",
    "processed_dataset = Dataset.from_pandas(pd.DataFrame(processed_dataset))\n",
    "processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 2031/2031 [00:04<00:00, 415.53 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example[\"messages\"])):\n",
    "        output_texts.append(\n",
    "            tokenizer.apply_chat_template(\n",
    "                example[\"messages\"][i],\n",
    "                tokenize=False,\n",
    "            )\n",
    "        )\n",
    "    return output_texts\n",
    "\n",
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        formatting_prompts_func(element),\n",
    "        truncation=False,\n",
    "        padding=False,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_length=False,\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": outputs[\"input_ids\"],\n",
    "        \"attention_mask\": outputs[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "# 데이터 토큰화\n",
    "tokenized_dataset = processed_dataset.map(\n",
    "    tokenize,\n",
    "    remove_columns=list(processed_dataset.features),\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    load_from_cache_file=True,\n",
    "    desc=\"Tokenizing\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 2031/2031 [00:01<00:00, 1114.52 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max token length: 1886\n",
      "min token length: 436\n",
      "avg token length: 968.5900383141762\n",
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "대한민국 수능 전문가이다. 수능 국어 지문과 질문 그리고 선택지가 주어졌을 때 정답을 맞출 수 있다.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "문제를 풀이할 때, 반드시 지문을 참고하세요.\n",
      "문제는 무조건 1개의 정답만 있습니다.\n",
      "문제를 풀이할 때 모든 선택지들을 검토하세요.\n",
      "모든 선택지마다 근거를 지문에서 찾아 설명하세요.\n",
      "\n",
      "다음의 형식을 따라 답변하세요.\n",
      "\n",
      "1번: (선택지 1번에 대한 답변) + \"(지문 속 근거가 된 문장)\"\n",
      "2번: (선택지 2번에 대한 답변) + \"(지문 속 근거가 된 문장)\"\n",
      "3번: (선택지 3번에 대한 답변) + \"(지문 속 근거가 된 문장)\"\n",
      "4번: (선택지 4번에 대한 답변) + \"(지문 속 근거가 된 문장)\"\n",
      "5번: (선택지 5번에 대한 답변) + \"(지문 속 근거가 된 문장)\"\n",
      "최종 정답: (근거 : 고른 정답에 대한 근거 # 정답 번호)\n",
      "\n",
      "지문 : \n",
      "부산 강서구 김해공항 인근의 ‘공항마을’ 등 그린벨트(개발제한구역)에서 풀린 집단취락(마을) 지역에 상가나 공장을 지을 수 있게 된다. 또 임대주택을 35% 이상 의무적으로 짓도록 한 규제가 사실상 사라지는 등 그린벨트 내 규제가 대폭 완화된다. 국토교통부는 그린벨트 해제지역의 개발 사업을 활성화하기 위해 ‘개발제한구역의 조정을 위한 도시관리계획 변경안 수립 지침’ 및 ‘도시·군관리계획수립지침’을 이같이 개정해 11일부터 시행한다고 10일 발표했다. 개정된 지침은 그린벨트에서 풀린 집단취락이 시가지나 공항, 항만, 철도역 등 거점시설과 맞닿아 있는 경우 상가나 공장을 지을 수 있도록 했다. 기존에는 자연녹지지역이나 주거지역으로만 개발이 허용돼 정비사업이 지연됨에 따라 주민의 생활 불편을 초래한다는 지적을 반영했다. 김정희 국토부 녹색도시과장은 “전국 해제 취락 1656개(106㎢) 가운데 정비가 완료되거나 진행 중인 곳은 171개(10%)에 불과하다”며 “이제 부산 공항마을에도 김포공항 인근의 아울렛 같은 쇼핑시설이 들어설 수 있게 돼 정비사업에 속도가 붙을 것”이라고 말했다. 개정된 지침은 또 그린벨트에서 해제된 땅에 택지개발사업, 공공주택사업 등을 통해 주택을 지을 때 임대주택 용지가 6개월 넘게 팔리지 않으면 분양주택 용지로 바꿀 수 있도록 했다. 의무적으로 임대주택을 35% 이상 공급해야 하는 규정을 지키지 않아도 되는 것이다. 이럴 경우 창원 대전 등의 주택사업에 속도가 붙을 전망이다. 민간업체들의 개발 사업 참여를 장려하는 방안도 담겼다. 민간이 그린벨트 해제 대상 지역 개발을 위해 설립된 특수목적법인에 출자할 수 있는 비율을 2015년까지 한시적으로 종전 2분의 1 미만에서 3분의 2 미만으로 확대하기로 했다.\n",
      "\n",
      "질문 : \n",
      "그린벨트 해제지역에서 상가나 공장을 지을 수 있도록 한 개정된 지침의 시행일은 언제인가?\n",
      "\n",
      "선택지 : \n",
      "1 - 11일\n",
      "2 - 10일\n",
      "3 - 1일\n",
      "4 - 12일\n",
      "5 - 15일\n",
      "\n",
      "\n",
      "1번, 2번, 3번, 4번, 5번 중에 하나를 정답으로 고르고, 근거와 고른 정답을 작성하세요.\n",
      "근거와 정답은 \"#\"으로 구분하세요.\n",
      "근거 :<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "근거 : 개정된 지침은 11일부터 시행된다고 국토교통부가 10일 발표했다. 따라서 그린벨트 해제지역에서 상가나 공장을 지을 수 있도록 한 개정된 지침의 시행일은 11일이다. # 1<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 데이터 분리\n",
    "tokenized_dataset = tokenized_dataset.filter(lambda x: len(x[\"input_ids\"]) <= 2048)  \n",
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.10, seed=42)\n",
    "\n",
    "train_dataset = tokenized_dataset['train']\n",
    "eval_dataset = tokenized_dataset['test']\n",
    "\n",
    "\n",
    "train_dataset_token_lengths = [len(train_dataset[i][\"input_ids\"]) for i in range(len(train_dataset))]\n",
    "print(f\"max token length: {max(train_dataset_token_lengths)}\")\n",
    "print(f\"min token length: {min(train_dataset_token_lengths)}\")\n",
    "print(f\"avg token length: {np.mean(train_dataset_token_lengths)}\")\n",
    "\n",
    "# 데이터 확인\n",
    "print(tokenizer.decode(train_dataset[0][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_template = \"assistant<|end_header_id|>\"\n",
    "data_collator = DataCollatorForCompletionOnlyLM(\n",
    "    response_template=response_template,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    logits = logits if not isinstance(logits, tuple) else logits[0]\n",
    "    logit_idx = [tokenizer.vocab[\"1\"],\n",
    "                    tokenizer.vocab[\"2\"],\n",
    "                    tokenizer.vocab[\"3\"],\n",
    "                    tokenizer.vocab[\"4\"], \n",
    "                    tokenizer.vocab[\"5\"]]\n",
    "    logits = logits[:, -2, logit_idx] # -2: answer token, -1: eos token\n",
    "    return logits\n",
    "\n",
    "\n",
    "    # metric 계산 함수\n",
    "def compute_metrics(evaluation_result):\n",
    "    logits, labels = evaluation_result\n",
    "    int_output_map = {\"1\": 0, \"2\": 1, \"3\": 2, \"4\": 3, \"5\": 4}\n",
    "\n",
    "\n",
    "    # 토큰화된 레이블 디코딩\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    labels = [label.split('#')[-1].strip() for label in labels]\n",
    "    labels = [int_output_map.get(label, -1) for label in labels] \n",
    "\n",
    "    # 소프트맥스 함수를 사용하여 로그트 변환\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(logits, dtype=torch.float32), dim=-1)\n",
    "\n",
    "    predictions = np.argmax(probs, axis=-1)\n",
    "\n",
    "    # 정확도 계산\n",
    "    acc = acc_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "\n",
    "    return {\"accuracy\": acc[\"accuracy\"], \"f1\": f1[\"f1\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '0.13.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=f\"./outputs + {model_name.split('/')[-1]}\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    save_only_model=True,\n",
    "    report_to=\"none\",\n",
    "    fp16 = True\n",
    ")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics = preprocess_logits_for_metrics,\n",
    "    args=sft_config,\n",
    "    max_seq_length=2048,\n",
    "    packing=False,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='310' max='1368' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 310/1368 12:06 < 41:36, 0.42 it/s, Epoch 0.68/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for param in model.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "# TODO Test Data 경로 입력\n",
    "test_df = pd.read_csv('datas/test.csv')\n",
    "\n",
    "# Flatten the JSON dataset\n",
    "records = []\n",
    "for _, row in test_df.iterrows():\n",
    "    problems = literal_eval(row['problems'])\n",
    "    record = {\n",
    "        'id': row['id'],\n",
    "        'paragraph': row['paragraph'],\n",
    "        'question': problems['question'],\n",
    "        'choices': problems['choices'],\n",
    "        'answer': problems.get('answer', None),\n",
    "        \"question_plus\": problems.get('question_plus', None),\n",
    "    }\n",
    "    # Include 'question_plus' if it exists\n",
    "    if 'question_plus' in problems:\n",
    "        record['question_plus'] = problems['question_plus']\n",
    "    records.append(record)\n",
    "        \n",
    "# Convert to DataFrame\n",
    "test_df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/869 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "  1%|          | 10/869 [00:35<49:40,  3.47s/it] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 869/869 [52:05<00:00,  3.60s/it] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "model.to('cuda:0')\n",
    "test_dataset = []\n",
    "for i, row in test_df.iterrows():\n",
    "    choices_string = \"\\n\".join([f\"{idx + 1} - {choice}\" for idx, choice in enumerate(row[\"choices\"])])\n",
    "    len_choices = len(row[\"choices\"])\n",
    "    \n",
    "    # <보기>가 있을 때\n",
    "    if row[\"question_plus\"]:\n",
    "        user_message = PROMPT_QUESTION_PLUS.format(\n",
    "            paragraph=row[\"paragraph\"],\n",
    "            question=row[\"question\"],\n",
    "            question_plus=row[\"question_plus\"],\n",
    "            choices=choices_string,\n",
    "        )\n",
    "    # <보기>가 없을 때\n",
    "    else:\n",
    "        user_message = PROMPT_NO_QUESTION_PLUS.format(\n",
    "            paragraph=row[\"paragraph\"],\n",
    "            question=row[\"question\"],\n",
    "            choices=choices_string,\n",
    "        )\n",
    "\n",
    "    test_dataset.append(\n",
    "        {\n",
    "            \"id\": row[\"id\"],\n",
    "            \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"\"\"대한민국 수능 전문가이다. 수능 국어 지문과 질문 그리고 선택지가 주어졌을 때 정답을 맞출 수 있다.\"\"\"},\n",
    "            {\"role\": \"user\", \"content\": user_message},\n",
    "            ],\n",
    "            \"label\": row[\"answer\"],\n",
    "            \"len_choices\": len_choices,\n",
    "        }\n",
    "    )\n",
    "\n",
    "generated_infer_results = []\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    for idx, data in enumerate(tqdm(test_dataset)):\n",
    "        _id = data[\"id\"]\n",
    "        messages = data[\"messages\"]\n",
    "        len_choices = data[\"len_choices\"]\n",
    "\n",
    "        # 텍스트 생성을 위한 입력 데이터 준비\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(\"cuda\")\n",
    "        # 모델을 이용한 텍스트 생성\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_new_tokens=200,  # 최대 생성 토큰 수\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "        # 생성된 텍스트 디코딩\n",
    "        generated_text = tokenizer.batch_decode(\n",
    "            outputs[:,inputs.shape[1]:], skip_special_tokens=True)[0]\n",
    "        generated_text\n",
    "        # 생성된 텍스트와 라벨을 결과 리스트에 추가\n",
    "        generated_infer_results.append({\n",
    "            \"id\": _id,  # 고유 ID\n",
    "            \"answer\": generated_text,  # 생성된 텍스트\n",
    "            \"label\": data[\"label\"]  # 실제 라벨이 있다면, data에서 가져옴\n",
    "        })\n",
    "generated_infer_results = pd.DataFrame(generated_infer_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = generated_infer_results['answer'].apply(lambda x: x.split('#')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['len'] = test['answer'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_last_number(text):\n",
    "    numbers = re.findall(r'\\d+', text) # 모든 숫자 추출\n",
    "    if int(numbers[-1]) > 5:\n",
    "        numbers = [1]\n",
    "    return int(numbers[-1]) if numbers else None\n",
    "\n",
    "generated_infer_results['answer'] = generated_infer_results['answer'].apply(extract_last_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m generated_infer_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mgenerated_infer_results\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43manswer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextract_last_number\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[44], line 3\u001b[0m, in \u001b[0;36mextract_last_number\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_last_number\u001b[39m(text):\n\u001b[0;32m----> 3\u001b[0m     numbers \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43md+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 모든 숫자 추출\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mint\u001b[39m(numbers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m5\u001b[39m:\n\u001b[1;32m      5\u001b[0m         numbers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/re.py:240\u001b[0m, in \u001b[0;36mfindall\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfindall\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    233\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of all non-overlapping matches in the string.\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \n\u001b[1;32m    235\u001b[0m \u001b[38;5;124;03m    If one or more capturing groups are present in the pattern, return\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    238\u001b[0m \n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m    Empty matches are included in the result.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "generated_infer_results['answer'] = generated_infer_results['answer'].apply(extract_last_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_infer_results['answer'] = generated_infer_results['answer'].apply(lambda x: 1 if x > 5 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_infer_results[['id', 'answer']].to_csv('output.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>generation-for-nlp-0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>generation-for-nlp-1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>generation-for-nlp-2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>generation-for-nlp-3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>generation-for-nlp-4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>generation-for-nlp-1609</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>generation-for-nlp-1512</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>generation-for-nlp-1382</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>generation-for-nlp-702</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>generation-for-nlp-1404</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>869 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id  answer\n",
       "0       generation-for-nlp-0       3\n",
       "1       generation-for-nlp-1       2\n",
       "2       generation-for-nlp-2       4\n",
       "3       generation-for-nlp-3       1\n",
       "4       generation-for-nlp-4       1\n",
       "..                       ...     ...\n",
       "864  generation-for-nlp-1609       1\n",
       "865  generation-for-nlp-1512       1\n",
       "866  generation-for-nlp-1382       3\n",
       "867   generation-for-nlp-702       3\n",
       "868  generation-for-nlp-1404       1\n",
       "\n",
       "[869 rows x 2 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
