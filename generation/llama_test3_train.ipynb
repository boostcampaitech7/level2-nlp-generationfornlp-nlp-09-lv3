{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipykernel in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (6.29.5)\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (8.1.5)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (4.46.3)\n",
      "Requirement already satisfied: trl in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (1.5.2)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (3.9.2)\n",
      "Requirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (0.4.3)\n",
      "Requirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (0.13.2)\n",
      "Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (0.44.1)\n",
      "Requirement already satisfied: comm>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r requirements.txt (line 1)) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r requirements.txt (line 1)) (1.6.7)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r requirements.txt (line 1)) (8.15.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r requirements.txt (line 1)) (7.3.4)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r requirements.txt (line 1)) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r requirements.txt (line 1)) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r requirements.txt (line 1)) (1.6.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r requirements.txt (line 1)) (23.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r requirements.txt (line 1)) (5.9.0)\n",
      "Requirement already satisfied: pyzmq>=24 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r requirements.txt (line 1)) (25.1.2)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r requirements.txt (line 1)) (6.1)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r requirements.txt (line 1)) (5.7.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /opt/conda/lib/python3.10/site-packages (from ipywidgets->-r requirements.txt (line 2)) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /opt/conda/lib/python3.10/site-packages (from ipywidgets->-r requirements.txt (line 2)) (3.0.13)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 3)) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 3)) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 3)) (1.26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 3)) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 3)) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 3)) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 3)) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 3)) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 3)) (4.67.0)\n",
      "Requirement already satisfied: accelerate>=0.34.0 in /opt/conda/lib/python3.10/site-packages (from trl->-r requirements.txt (line 4)) (1.1.1)\n",
      "Requirement already satisfied: datasets>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from trl->-r requirements.txt (line 4)) (3.1.0)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from trl->-r requirements.txt (line 4)) (13.9.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 5)) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 6)) (4.55.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 6)) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 6)) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 6)) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 6)) (2.9.0)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate->-r requirements.txt (line 7)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate->-r requirements.txt (line 7)) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate->-r requirements.txt (line 7)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate->-r requirements.txt (line 7)) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate->-r requirements.txt (line 7)) (2023.9.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft->-r requirements.txt (line 8)) (2.1.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl->-r requirements.txt (line 4)) (18.0.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl->-r requirements.txt (line 4)) (3.11.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers->-r requirements.txt (line 3)) (4.7.1)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (0.18.1)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (3.0.36)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (1.0.4)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (4.8.0)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel->-r requirements.txt (line 1)) (0.4)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->-r requirements.txt (line 1)) (4.3.6)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 3)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 3)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 3)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 3)) (2024.8.30)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft->-r requirements.txt (line 8)) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft->-r requirements.txt (line 8)) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft->-r requirements.txt (line 8)) (3.1.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate->-r requirements.txt (line 7)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate->-r requirements.txt (line 7)) (2024.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl->-r requirements.txt (line 4)) (3.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl->-r requirements.txt (line 4)) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl->-r requirements.txt (line 4)) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl->-r requirements.txt (line 4)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl->-r requirements.txt (line 4)) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl->-r requirements.txt (line 4)) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl->-r requirements.txt (line 4)) (1.17.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl->-r requirements.txt (line 4)) (5.0.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (0.8.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl->-r requirements.txt (line 4)) (0.1.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (0.2.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft->-r requirements.txt (line 8)) (2.1.1)\n",
      "Requirement already satisfied: executing in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (0.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft->-r requirements.txt (line 8)) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.3)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement BitsAndBytesConfig (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for BitsAndBytesConfig\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.46.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import transformers\n",
    "from ast import literal_eval\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import evaluate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq, DataCollatorForLanguageModeling\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# 1. 설정: pandas 출력 옵션 및 시드 고정\n",
    "pd.set_option('display.max_columns', None)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b63d8066b8f402390e3abeb910b0563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. 모델 및 토크나이저 로드 (8-bit 양자화)\n",
    "model_name = \"sh2orc/Llama-3.1-Korean-8B-Instruct\"\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_8bit=True,            # 8-bit 정밀도로 모델 로드\n",
    "#     llm_int8_threshold=6.0,       # int8 양자화 임계값\n",
    "#     llm_int8_has_fp16_weight=False # FP16 가중치 사용 여부\n",
    "# )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 3. PEFT 설정 (LoRA)\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=['q_proj', 'v_proj'],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/data/data/train.csv'  # Update this path as needed\n",
    "dataset = pd.read_csv(dataset_path) \n",
    "\n",
    "# Flatten the JSON dataset\n",
    "records = []\n",
    "for _, row in dataset.iterrows():\n",
    "    problems = literal_eval(row['problems'])\n",
    "    record = {\n",
    "        'id': row['id'],\n",
    "        'paragraph': row['paragraph'],\n",
    "        'question': problems['question'],\n",
    "        'choices': problems['choices'],\n",
    "        'answer': problems.get('answer', None),\n",
    "        \"question_plus\": problems.get('question_plus', None),\n",
    "    }\n",
    "    # Include 'question_plus' if it exists\n",
    "    if 'question_plus' in problems:\n",
    "        record['question_plus'] = problems['question_plus']\n",
    "    records.append(record)\n",
    "        \n",
    "\n",
    "df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "고려 문종 때에 남경(南京)으로 승격되었다.\n"
     ]
    }
   ],
   "source": [
    "answer_num = df.iloc[1][\"answer\"]\n",
    "choices = df.iloc[1][\"choices\"]\n",
    "\n",
    "# if isinstance(answer_num, int) and 1 <= answer_num <= 5:\n",
    "#         output = choices[answer_num - 1]  # 정답 텍스트\n",
    "# else:\n",
    "#     output = \"정답 없음\" \n",
    "    \n",
    "output = choices[answer_num - 1]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트 템플릿 정의\n",
    "PROMPT_TEMPLATE = \"\"\"다음 지문을 읽고 질문에 답하세요.\n",
    "\n",
    "지문:\n",
    "{paragraph}\n",
    "\n",
    "질문:\n",
    "{question}\n",
    "\n",
    "{question_plus}\n",
    "선택지:\n",
    "{choices}\n",
    "\n",
    "정답을 선택지 중에서 선택하여 작성하세요:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = []\n",
    "for i in range(len(df)):\n",
    "    choices = df.iloc[i][\"choices\"]\n",
    "    choices_string = \"\\n\".join([f\"{idx + 1}. {choice}\" for idx, choice in enumerate(choices)])\n",
    "    paragraph = df.iloc[i][\"paragraph\"]\n",
    "    question = df.iloc[i][\"question\"]\n",
    "    answer_num = df.iloc[i][\"answer\"]\n",
    "    question_plus = df.iloc[i].get(\"question_plus\", None)\n",
    "\n",
    "    # 정답 번호에 대응하는 텍스트\n",
    "    # if isinstance(answer_num, int) and 1 <= answer_num <= len(choices):\n",
    "    #     output = choices[answer_num - 1]  # 정답 텍스트\n",
    "    # else:\n",
    "    #     output = \"정답 없음\"  # 정답이 없거나 유효하지 않을 경우\n",
    "\n",
    "    output = choices[answer_num - 1]\n",
    "    \n",
    "    # 입력 데이터 생성\n",
    "    input_text = PROMPT_TEMPLATE.format(\n",
    "        paragraph=paragraph,\n",
    "        question=question,\n",
    "        question_plus=f\"<보기>:\\n{question_plus}\" if question_plus else \"\",  # 'question_plus' 처리\n",
    "        choices=choices_string,\n",
    "    )\n",
    "\n",
    "    processed_dataset.append(\n",
    "        {\n",
    "            \"input\": input_text,\n",
    "            \"output\": output,\n",
    "        }\n",
    "    )\n",
    "\n",
    "processed_df = pd.DataFrame(processed_dataset)\n",
    "processed_dataset = Dataset.from_pandas(processed_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "797\n"
     ]
    }
   ],
   "source": [
    "#데이터의 최대 길이 계산\n",
    "max_num = 0\n",
    "\n",
    "for _ in range(len(processed_dataset)):\n",
    "    data_length = len(processed_dataset[i][\"input\"] + processed_dataset[i][\"output\"])\n",
    "    if data_length > max_num:\n",
    "        max_num = data_length\n",
    "        \n",
    "print(max_num)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eccbcf702bc4d8193596f7057247b5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2031 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "\n",
    "   \n",
    "    full_texts = [inp + tokenizer.eos_token + out for inp, out in zip(inputs, outputs)]\n",
    "\n",
    "    \n",
    "    tokenized_inputs = tokenizer(\n",
    "        full_texts,\n",
    "        max_length=1024,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "   \n",
    "    labels = tokenized_inputs[\"input_ids\"].copy()\n",
    "\n",
    "    \n",
    "    for i in range(len(inputs)):\n",
    "        input_ids = tokenizer(\n",
    "            inputs[i],\n",
    "            max_length=1024,\n",
    "            truncation=True,\n",
    "        )[\"input_ids\"]\n",
    "        input_len = len(input_ids)\n",
    "        \n",
    "        \n",
    "        labels[i] = torch.tensor(labels[i]) \n",
    "        labels[i][:input_len] = -100 \n",
    "        labels[i] = labels[i].tolist()  \n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "tokenized_datasets = processed_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=processed_dataset.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = tokenized_datasets['train']\n",
    "eval_dataset = tokenized_datasets['test']\n",
    "\n",
    "# 7. 데이터 콜레이터 정의\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    return metric.compute(predictions=decoded_preds, references=decoded_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_661913/3139246749.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./outputs_llama\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=4,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# 10. Trainer 초기화 및 훈련\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1368' max='1368' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1368/1368 1:10:40, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.955400</td>\n",
       "      <td>1.876130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.897700</td>\n",
       "      <td>1.863594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 15. 훈련 시작\n",
    "trainer.train()\n",
    "trainer.save_model(\"./fine_tuned_llama8B_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 추론단계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rank-bm25\n",
      "  Obtaining dependency information for rank-bm25 from https://files.pythonhosted.org/packages/2a/21/f691fb2613100a62b3fa91e9988c991e9ca5b89ea31c0d3152a3210344f9/rank_bm25-0.2.2-py3-none-any.whl.metadata\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rank-bm25) (1.26.0)\n",
      "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Installing collected packages: rank-bm25\n",
      "Successfully installed rank-bm25-0.2.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install rank-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import transformers\n",
    "from ast import literal_eval\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import evaluate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq, DataCollatorForLanguageModeling,DataCollatorWithPadding\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from rank_bm25 import BM25Okapi\n",
    "import random\n",
    "\n",
    "\n",
    "# 1. 설정: pandas 출력 옵션 및 시드 고정\n",
    "pd.set_option('display.max_columns', None)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1cacacc53954428b82cda27f83cc1c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# 2. 모델 및 토크나이저 로드\n",
    "model_name = \"./fine_tuned_llama8B_model\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# 3. PEFT 설정 (LoRA)\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=['q_proj', 'v_proj'],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 모델을 평가 모드로 설정\n",
    "model.eval()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "# 데이터 로드\n",
    "dataset_path = '/data/data/test.csv'  # 필요한 경로로 업데이트하세요\n",
    "dataset = pd.read_csv(dataset_path)\n",
    "\n",
    "# JSON 형태로 저장된 'problems' 컬럼을 펼치기\n",
    "records = []\n",
    "for _, row in dataset.iterrows():\n",
    "    problems = literal_eval(row['problems'])\n",
    "    record = {\n",
    "        'id': row['id'],\n",
    "        'paragraph': row['paragraph'],\n",
    "        'question': problems['question'],\n",
    "        'choices': problems['choices'],\n",
    "        \"question_plus\": problems.get('question_plus', None),\n",
    "    }\n",
    "    records.append(record)\n",
    "\n",
    "df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트 템플릿 정의\n",
    "PROMPT_TEMPLATE = \"\"\"다음 지문을 읽고 질문에 답하세요.\n",
    "\n",
    "지문:\n",
    "{paragraph}\n",
    "\n",
    "질문:\n",
    "{question}\n",
    "\n",
    "{question_plus}\n",
    "선택지:\n",
    "{choices}\n",
    "\n",
    "정답을 선택지 중에서 선택하여 작성하세요:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = []\n",
    "for i in range(len(df)):\n",
    "    choices = df.iloc[i][\"choices\"]\n",
    "    choices_string = \"\\n\".join([f\"{idx + 1}. {choice}\" for idx, choice in enumerate(choices)])\n",
    "    paragraph = df.iloc[i][\"paragraph\"]\n",
    "    question = df.iloc[i][\"question\"]\n",
    "    question_plus = df.iloc[i].get(\"question_plus\", None)\n",
    "\n",
    "    # 입력 데이터 생성\n",
    "    input_text = PROMPT_TEMPLATE.format(\n",
    "        paragraph=paragraph,\n",
    "        question=question,\n",
    "        question_plus=f\"<보기>:\\n{question_plus}\" if question_plus else \"\",\n",
    "        choices=choices_string,\n",
    "    )\n",
    "\n",
    "    processed_dataset.append(\n",
    "        {\n",
    "            \"id\": df.iloc[i][\"id\"],\n",
    "            \"input\": input_text,\n",
    "            \"choices\": choices,\n",
    "        }\n",
    "    )\n",
    "\n",
    "processed_df = pd.DataFrame(processed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 단위 설정\n",
    "batch_size = 1  # 원하는 배치 크기로 설정\n",
    "\n",
    "# 텍스트 생성 함수 정의 (배치 처리)\n",
    "def generate_answers(input_texts):\n",
    "    inputs = tokenizer(\n",
    "        input_texts,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=1024,\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 답변 생성\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=1024 + 512,  # 입력 길이 + 예상 답변 길이\n",
    "            num_beams=5,\n",
    "            early_stopping=True,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            no_repeat_ngram_size=2,\n",
    "        )\n",
    "\n",
    "    # 생성된 텍스트 디코딩\n",
    "    generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    # 입력 텍스트 부분을 제외하고 답변만 추출\n",
    "    generated_answers = []\n",
    "    for input_text, generated_text in zip(input_texts, generated_texts):\n",
    "        answer = generated_text[len(input_text):].strip()\n",
    "        generated_answers.append(answer)\n",
    "\n",
    "    return generated_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유사도 계산 및 가장 유사한 선택지 선택 함수 정의\n",
    "def select_best_choices(generated_answers, choices_list):\n",
    "    selected_indices = []\n",
    "    for generated_answer, choices in zip(generated_answers, choices_list):\n",
    "        # 선택지와 생성된 답변을 토큰화\n",
    "        tokenized_choices = [tokenizer.tokenize(choice) for choice in choices]\n",
    "        tokenized_answer = tokenizer.tokenize(generated_answer)\n",
    "\n",
    "        # BM25 객체 생성\n",
    "        bm25 = BM25Okapi(tokenized_choices)\n",
    "\n",
    "        # 유사도 스코어 계산\n",
    "        scores = bm25.get_scores(tokenized_answer)\n",
    "\n",
    "        # 가장 높은 스코어의 선택지 선택\n",
    "        best_choice_index = int(np.argmax(scores))\n",
    "        selected_indices.append(best_choice_index + 1)  # 선택지는 1부터 시작하므로 +1\n",
    "\n",
    "    return selected_indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전체 추론 후 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/109 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  1%|          | 1/109 [01:35<2:52:01, 95.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  2%|▏         | 2/109 [03:10<2:50:05, 95.38s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  3%|▎         | 3/109 [04:47<2:49:43, 96.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  4%|▎         | 4/109 [06:24<2:48:39, 96.37s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  5%|▍         | 5/109 [08:00<2:47:05, 96.40s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  6%|▌         | 6/109 [11:48<4:02:11, 141.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  6%|▋         | 7/109 [13:24<3:34:32, 126.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  7%|▋         | 8/109 [15:00<3:16:23, 116.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  8%|▊         | 9/109 [16:36<3:03:39, 110.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  9%|▉         | 10/109 [18:12<2:54:36, 105.82s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 10%|█         | 11/109 [19:49<2:48:21, 103.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 11%|█         | 12/109 [24:24<4:11:00, 155.26s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 12%|█▏        | 13/109 [28:58<5:05:58, 191.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 13%|█▎        | 14/109 [30:34<4:17:39, 162.73s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 14%|█▍        | 15/109 [32:10<3:43:11, 142.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 15%|█▍        | 16/109 [33:47<3:19:43, 128.86s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 16%|█▌        | 17/109 [35:24<3:02:53, 119.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 17%|█▋        | 18/109 [37:00<2:50:22, 112.33s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 17%|█▋        | 19/109 [41:17<3:53:30, 155.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 18%|█▊        | 20/109 [45:43<4:39:56, 188.72s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 19%|█▉        | 21/109 [49:27<4:52:38, 199.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 20%|██        | 22/109 [51:03<4:04:04, 168.33s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 21%|██        | 23/109 [52:39<3:29:55, 146.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 22%|██▏       | 24/109 [54:15<3:06:24, 131.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 23%|██▎       | 25/109 [55:53<2:49:52, 121.33s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 24%|██▍       | 26/109 [57:30<2:37:40, 113.98s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 25%|██▍       | 27/109 [1:00:06<2:53:15, 126.78s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 26%|██▌       | 28/109 [1:04:34<3:48:23, 169.18s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 27%|██▋       | 29/109 [1:09:03<4:25:30, 199.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 28%|██▊       | 30/109 [1:13:41<4:53:01, 222.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 28%|██▊       | 31/109 [1:18:24<5:12:55, 240.71s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 29%|██▉       | 32/109 [1:22:47<5:17:33, 247.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 30%|███       | 33/109 [1:27:29<5:26:32, 257.80s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 31%|███       | 34/109 [1:31:45<5:21:44, 257.39s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 32%|███▏      | 35/109 [1:35:41<5:09:21, 250.83s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 33%|███▎      | 36/109 [1:40:27<5:18:05, 261.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 34%|███▍      | 37/109 [1:45:10<5:21:32, 267.95s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 35%|███▍      | 38/109 [1:49:49<5:20:54, 271.19s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 36%|███▌      | 39/109 [1:52:51<4:45:23, 244.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 37%|███▋      | 40/109 [1:56:20<4:28:47, 233.73s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 38%|███▊      | 41/109 [2:00:27<4:29:37, 237.90s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 39%|███▊      | 42/109 [2:04:56<4:35:47, 246.98s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 39%|███▉      | 43/109 [2:09:20<4:37:22, 252.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 40%|████      | 44/109 [2:13:19<4:28:49, 248.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 41%|████▏     | 45/109 [2:17:57<4:34:31, 257.37s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 42%|████▏     | 46/109 [2:19:33<3:39:14, 208.81s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 43%|████▎     | 47/109 [2:21:10<3:01:00, 175.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 44%|████▍     | 48/109 [2:22:47<2:34:27, 151.92s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 45%|████▍     | 49/109 [2:24:24<2:15:22, 135.38s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 46%|████▌     | 50/109 [2:26:00<2:01:23, 123.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 47%|████▋     | 51/109 [2:30:35<2:43:26, 169.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 48%|████▊     | 52/109 [2:35:17<3:12:42, 202.84s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 49%|████▊     | 53/109 [2:40:04<3:32:57, 228.16s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 50%|████▉     | 54/109 [2:42:32<3:07:07, 204.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 50%|█████     | 55/109 [2:44:10<2:34:58, 172.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 51%|█████▏    | 56/109 [2:45:48<2:12:23, 149.88s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 52%|█████▏    | 57/109 [2:47:25<1:56:12, 134.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 53%|█████▎    | 58/109 [2:49:02<1:44:25, 122.85s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 54%|█████▍    | 59/109 [2:50:38<1:35:48, 114.97s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 55%|█████▌    | 60/109 [2:52:15<1:29:34, 109.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 56%|█████▌    | 61/109 [2:53:53<1:24:45, 105.95s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 57%|█████▋    | 62/109 [2:57:24<1:47:45, 137.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 58%|█████▊    | 63/109 [3:00:00<1:49:36, 142.96s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 59%|█████▊    | 64/109 [3:01:42<1:38:03, 130.74s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 60%|█████▉    | 65/109 [3:03:19<1:28:32, 120.73s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 61%|██████    | 66/109 [3:04:57<1:21:29, 113.71s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 61%|██████▏   | 67/109 [3:06:58<1:21:11, 116.00s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 62%|██████▏   | 68/109 [3:08:35<1:15:28, 110.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 63%|██████▎   | 69/109 [3:10:13<1:11:07, 106.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 64%|██████▍   | 70/109 [3:12:26<1:14:29, 114.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 65%|██████▌   | 71/109 [3:14:56<1:19:18, 125.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 66%|██████▌   | 72/109 [3:16:34<1:12:05, 116.90s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 67%|██████▋   | 73/109 [3:18:11<1:06:32, 110.89s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 68%|██████▊   | 74/109 [3:19:48<1:02:16, 106.75s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 69%|██████▉   | 75/109 [3:21:26<58:58, 104.06s/it]  Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 70%|██████▉   | 76/109 [3:23:02<56:01, 101.87s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 71%|███████   | 77/109 [3:24:40<53:40, 100.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 72%|███████▏  | 78/109 [3:26:17<51:29, 99.65s/it] Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 72%|███████▏  | 79/109 [3:27:56<49:36, 99.21s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 73%|███████▎  | 80/109 [3:31:40<1:06:02, 136.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 74%|███████▍  | 81/109 [3:35:12<1:14:24, 159.43s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 75%|███████▌  | 82/109 [3:36:49<1:03:20, 140.77s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 76%|███████▌  | 83/109 [3:40:36<1:12:07, 166.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 77%|███████▋  | 84/109 [3:42:50<1:05:19, 156.78s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 78%|███████▊  | 85/109 [3:44:27<55:32, 138.84s/it]  Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 79%|███████▉  | 86/109 [3:46:04<48:25, 126.31s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 80%|███████▉  | 87/109 [3:47:41<43:05, 117.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 81%|████████  | 88/109 [3:50:40<47:35, 135.96s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 82%|████████▏ | 89/109 [3:52:17<41:24, 124.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 83%|████████▎ | 90/109 [3:53:54<36:47, 116.16s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 83%|████████▎ | 91/109 [3:57:26<43:26, 144.79s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 84%|████████▍ | 92/109 [3:59:58<41:41, 147.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 85%|████████▌ | 93/109 [4:02:09<37:55, 142.19s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 86%|████████▌ | 94/109 [4:03:46<32:09, 128.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 87%|████████▋ | 95/109 [4:05:24<27:52, 119.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 88%|████████▊ | 96/109 [4:07:14<25:14, 116.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 89%|████████▉ | 97/109 [4:09:30<24:29, 122.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 90%|████████▉ | 98/109 [4:11:07<21:03, 114.83s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 91%|█████████ | 99/109 [4:12:45<18:15, 109.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 92%|█████████▏| 100/109 [4:14:22<15:54, 106.02s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 93%|█████████▎| 101/109 [4:15:59<13:46, 103.31s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 94%|█████████▎| 102/109 [4:17:37<11:51, 101.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 94%|█████████▍| 103/109 [4:19:14<10:02, 100.38s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 95%|█████████▌| 104/109 [4:20:55<08:22, 100.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 96%|█████████▋| 105/109 [4:22:36<06:41, 100.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 97%|█████████▋| 106/109 [4:24:13<04:58, 99.57s/it] Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 98%|█████████▊| 107/109 [4:25:50<03:17, 98.89s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 99%|█████████▉| 108/109 [4:27:28<01:38, 98.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "100%|██████████| 109/109 [4:29:50<00:00, 148.54s/it]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "# 배치 단위로 데이터 처리\n",
    "for idx in tqdm(range(0, len(processed_df), batch_size)):\n",
    "    batch = processed_df.iloc[idx:idx+batch_size]\n",
    "    input_texts = batch['input'].tolist()\n",
    "    choices_list = batch['choices'].tolist()\n",
    "    ids = batch['id'].tolist()\n",
    "\n",
    "    # 모델을 사용하여 답변 생성\n",
    "    generated_answers = generate_answers(input_texts)\n",
    "\n",
    "    # 가장 유사한 선택지 선택\n",
    "    answer_indices = select_best_choices(generated_answers, choices_list)\n",
    "\n",
    "    # 결과 저장\n",
    "    for id_, answer_index in zip(ids, answer_indices):\n",
    "        results.append({\n",
    "            'id': id_,\n",
    "            'answer': answer_index,\n",
    "        })\n",
    "\n",
    "# 결과를 DataFrame으로 변환\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# CSV로 저장\n",
    "output_path = 'output.csv'  # 필요한 경로로 업데이트하세요\n",
    "results_df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한행만 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     id  answer\n",
      "0  generation-for-nlp-0       4\n",
      "[\"다시 한 번 읽어보세요. 윗 글에서 가장 중요하게 강조한 내용은 '독자'와 '책'의 관계에 관한 것이었습니다. 따라서, 이와 관련이 없는 선택지를 찾아야 합니다. 선택지는 총 다섯 가지로 구성되어 있습니다. 첫 번째는 '같은 책'에 대한 내용입니다. 두 번째와 세 번째, 그리고 네 번째가 모두 독자를 중심으로 한 내용이기 때문에, 가장 적합한 선택지가 될 가능성이 높습니다. 다만, '읽기' 환경과 '과제'가 '배경' 지식을 '관점'보다 덜 중요하다고 할 수 있지만, 여전히 독자기능에 큰 영향을 미치는 요소들로 간주됩니다. 마지막으로, 책 속의 '사회'나 '시대'과 필자의 '대화'를 중점적으로 다루고 있으므로 '5'라는 번호를 선택하는 것이 가장 합리적인 선택일 것입니다.\"]\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터셋: 첫 번째 행만 선택\n",
    "test_batch = processed_df.iloc[:1]  # 첫 번째 행 선택\n",
    "input_texts = test_batch['input'].tolist()\n",
    "choices_list = test_batch['choices'].tolist()\n",
    "ids = test_batch['id'].tolist()\n",
    "\n",
    "# 모델을 사용하여 답변 생성\n",
    "generated_answers = generate_answers(input_texts)\n",
    "\n",
    "# 가장 유사한 선택지 선택\n",
    "answer_indices = select_best_choices(generated_answers, choices_list)\n",
    "\n",
    "# 결과 저장\n",
    "results = []\n",
    "for id_, answer_index in zip(ids, answer_indices):\n",
    "    results.append({\n",
    "        'id': id_,\n",
    "        'answer': answer_index,\n",
    "    })\n",
    "\n",
    "# 결과 출력\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "print(generated_answers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
