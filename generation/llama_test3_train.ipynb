{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipykernel in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (6.29.5)\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (8.1.5)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (4.46.3)\n",
      "Requirement already satisfied: trl in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (1.5.2)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (3.9.2)\n",
      "Requirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (0.4.3)\n",
      "Requirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (0.13.2)\n",
      "Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (0.44.1)\n",
      "Requirement already satisfied: comm>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r requirements.txt (line 1)) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r requirements.txt (line 1)) (1.6.7)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r requirements.txt (line 1)) (8.15.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r requirements.txt (line 1)) (7.3.4)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r requirements.txt (line 1)) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r requirements.txt (line 1)) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r requirements.txt (line 1)) (1.6.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r requirements.txt (line 1)) (23.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r requirements.txt (line 1)) (5.9.0)\n",
      "Requirement already satisfied: pyzmq>=24 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r requirements.txt (line 1)) (25.1.2)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r requirements.txt (line 1)) (6.1)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /opt/conda/lib/python3.10/site-packages (from ipykernel->-r requirements.txt (line 1)) (5.7.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /opt/conda/lib/python3.10/site-packages (from ipywidgets->-r requirements.txt (line 2)) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /opt/conda/lib/python3.10/site-packages (from ipywidgets->-r requirements.txt (line 2)) (3.0.13)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 3)) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 3)) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 3)) (1.26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 3)) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 3)) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 3)) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 3)) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 3)) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 3)) (4.67.0)\n",
      "Requirement already satisfied: accelerate>=0.34.0 in /opt/conda/lib/python3.10/site-packages (from trl->-r requirements.txt (line 4)) (1.1.1)\n",
      "Requirement already satisfied: datasets>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from trl->-r requirements.txt (line 4)) (3.1.0)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from trl->-r requirements.txt (line 4)) (13.9.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 5)) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 6)) (4.55.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 6)) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 6)) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 6)) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 6)) (2.9.0)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate->-r requirements.txt (line 7)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate->-r requirements.txt (line 7)) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate->-r requirements.txt (line 7)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate->-r requirements.txt (line 7)) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate->-r requirements.txt (line 7)) (2023.9.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft->-r requirements.txt (line 8)) (2.1.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl->-r requirements.txt (line 4)) (18.0.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.21.0->trl->-r requirements.txt (line 4)) (3.11.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers->-r requirements.txt (line 3)) (4.7.1)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (0.18.1)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (3.0.36)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (1.0.4)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (4.8.0)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel->-r requirements.txt (line 1)) (0.4)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->-r requirements.txt (line 1)) (4.3.6)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 3)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 3)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 3)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 3)) (2024.8.30)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft->-r requirements.txt (line 8)) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft->-r requirements.txt (line 8)) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft->-r requirements.txt (line 8)) (3.1.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate->-r requirements.txt (line 7)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate->-r requirements.txt (line 7)) (2024.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl->-r requirements.txt (line 4)) (3.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl->-r requirements.txt (line 4)) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl->-r requirements.txt (line 4)) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl->-r requirements.txt (line 4)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl->-r requirements.txt (line 4)) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl->-r requirements.txt (line 4)) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl->-r requirements.txt (line 4)) (1.17.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.21.0->trl->-r requirements.txt (line 4)) (5.0.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (0.8.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl->-r requirements.txt (line 4)) (0.1.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (0.2.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft->-r requirements.txt (line 8)) (2.1.1)\n",
      "Requirement already satisfied: executing in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->-r requirements.txt (line 1)) (0.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft->-r requirements.txt (line 8)) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.3)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement BitsAndBytesConfig (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for BitsAndBytesConfig\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.46.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import transformers\n",
    "from ast import literal_eval\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import evaluate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq, DataCollatorForLanguageModeling\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# 1. ì„¤ì •: pandas ì¶œë ¥ ì˜µì…˜ ë° ì‹œë“œ ê³ ì •\n",
    "pd.set_option('display.max_columns', None)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b63d8066b8f402390e3abeb910b0563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (8-bit ì–‘ìí™”)\n",
    "model_name = \"sh2orc/Llama-3.1-Korean-8B-Instruct\"\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_8bit=True,            # 8-bit ì •ë°€ë„ë¡œ ëª¨ë¸ ë¡œë“œ\n",
    "#     llm_int8_threshold=6.0,       # int8 ì–‘ìí™” ì„ê³„ê°’\n",
    "#     llm_int8_has_fp16_weight=False # FP16 ê°€ì¤‘ì¹˜ ì‚¬ìš© ì—¬ë¶€\n",
    "# )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 3. PEFT ì„¤ì • (LoRA)\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=['q_proj', 'v_proj'],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/data/data/train.csv'  # Update this path as needed\n",
    "dataset = pd.read_csv(dataset_path) \n",
    "\n",
    "# Flatten the JSON dataset\n",
    "records = []\n",
    "for _, row in dataset.iterrows():\n",
    "    problems = literal_eval(row['problems'])\n",
    "    record = {\n",
    "        'id': row['id'],\n",
    "        'paragraph': row['paragraph'],\n",
    "        'question': problems['question'],\n",
    "        'choices': problems['choices'],\n",
    "        'answer': problems.get('answer', None),\n",
    "        \"question_plus\": problems.get('question_plus', None),\n",
    "    }\n",
    "    # Include 'question_plus' if it exists\n",
    "    if 'question_plus' in problems:\n",
    "        record['question_plus'] = problems['question_plus']\n",
    "    records.append(record)\n",
    "        \n",
    "\n",
    "df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê³ ë ¤ ë¬¸ì¢… ë•Œì— ë‚¨ê²½(å—äº¬)ìœ¼ë¡œ ìŠ¹ê²©ë˜ì—ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "answer_num = df.iloc[1][\"answer\"]\n",
    "choices = df.iloc[1][\"choices\"]\n",
    "\n",
    "# if isinstance(answer_num, int) and 1 <= answer_num <= 5:\n",
    "#         output = choices[answer_num - 1]  # ì •ë‹µ í…ìŠ¤íŠ¸\n",
    "# else:\n",
    "#     output = \"ì •ë‹µ ì—†ìŒ\" \n",
    "    \n",
    "output = choices[answer_num - 1]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜\n",
    "PROMPT_TEMPLATE = \"\"\"ë‹¤ìŒ ì§€ë¬¸ì„ ì½ê³  ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.\n",
    "\n",
    "ì§€ë¬¸:\n",
    "{paragraph}\n",
    "\n",
    "ì§ˆë¬¸:\n",
    "{question}\n",
    "\n",
    "{question_plus}\n",
    "ì„ íƒì§€:\n",
    "{choices}\n",
    "\n",
    "ì •ë‹µì„ ì„ íƒì§€ ì¤‘ì—ì„œ ì„ íƒí•˜ì—¬ ì‘ì„±í•˜ì„¸ìš”:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = []\n",
    "for i in range(len(df)):\n",
    "    choices = df.iloc[i][\"choices\"]\n",
    "    choices_string = \"\\n\".join([f\"{idx + 1}. {choice}\" for idx, choice in enumerate(choices)])\n",
    "    paragraph = df.iloc[i][\"paragraph\"]\n",
    "    question = df.iloc[i][\"question\"]\n",
    "    answer_num = df.iloc[i][\"answer\"]\n",
    "    question_plus = df.iloc[i].get(\"question_plus\", None)\n",
    "\n",
    "    # ì •ë‹µ ë²ˆí˜¸ì— ëŒ€ì‘í•˜ëŠ” í…ìŠ¤íŠ¸\n",
    "    # if isinstance(answer_num, int) and 1 <= answer_num <= len(choices):\n",
    "    #     output = choices[answer_num - 1]  # ì •ë‹µ í…ìŠ¤íŠ¸\n",
    "    # else:\n",
    "    #     output = \"ì •ë‹µ ì—†ìŒ\"  # ì •ë‹µì´ ì—†ê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•Šì„ ê²½ìš°\n",
    "\n",
    "    output = choices[answer_num - 1]\n",
    "    \n",
    "    # ì…ë ¥ ë°ì´í„° ìƒì„±\n",
    "    input_text = PROMPT_TEMPLATE.format(\n",
    "        paragraph=paragraph,\n",
    "        question=question,\n",
    "        question_plus=f\"<ë³´ê¸°>:\\n{question_plus}\" if question_plus else \"\",  # 'question_plus' ì²˜ë¦¬\n",
    "        choices=choices_string,\n",
    "    )\n",
    "\n",
    "    processed_dataset.append(\n",
    "        {\n",
    "            \"input\": input_text,\n",
    "            \"output\": output,\n",
    "        }\n",
    "    )\n",
    "\n",
    "processed_df = pd.DataFrame(processed_dataset)\n",
    "processed_dataset = Dataset.from_pandas(processed_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "797\n"
     ]
    }
   ],
   "source": [
    "#ë°ì´í„°ì˜ ìµœëŒ€ ê¸¸ì´ ê³„ì‚°\n",
    "max_num = 0\n",
    "\n",
    "for _ in range(len(processed_dataset)):\n",
    "    data_length = len(processed_dataset[i][\"input\"] + processed_dataset[i][\"output\"])\n",
    "    if data_length > max_num:\n",
    "        max_num = data_length\n",
    "        \n",
    "print(max_num)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eccbcf702bc4d8193596f7057247b5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2031 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "\n",
    "   \n",
    "    full_texts = [inp + tokenizer.eos_token + out for inp, out in zip(inputs, outputs)]\n",
    "\n",
    "    \n",
    "    tokenized_inputs = tokenizer(\n",
    "        full_texts,\n",
    "        max_length=1024,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "   \n",
    "    labels = tokenized_inputs[\"input_ids\"].copy()\n",
    "\n",
    "    \n",
    "    for i in range(len(inputs)):\n",
    "        input_ids = tokenizer(\n",
    "            inputs[i],\n",
    "            max_length=1024,\n",
    "            truncation=True,\n",
    "        )[\"input_ids\"]\n",
    "        input_len = len(input_ids)\n",
    "        \n",
    "        \n",
    "        labels[i] = torch.tensor(labels[i]) \n",
    "        labels[i][:input_len] = -100 \n",
    "        labels[i] = labels[i].tolist()  \n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "tokenized_datasets = processed_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=processed_dataset.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = tokenized_datasets['train']\n",
    "eval_dataset = tokenized_datasets['test']\n",
    "\n",
    "# 7. ë°ì´í„° ì½œë ˆì´í„° ì •ì˜\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    return metric.compute(predictions=decoded_preds, references=decoded_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_661913/3139246749.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./outputs_llama\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=4,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# 10. Trainer ì´ˆê¸°í™” ë° í›ˆë ¨\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1368' max='1368' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1368/1368 1:10:40, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.955400</td>\n",
       "      <td>1.876130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.897700</td>\n",
       "      <td>1.863594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 15. í›ˆë ¨ ì‹œì‘\n",
    "trainer.train()\n",
    "trainer.save_model(\"./fine_tuned_llama8B_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì¶”ë¡ ë‹¨ê³„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rank-bm25\n",
      "  Obtaining dependency information for rank-bm25 from https://files.pythonhosted.org/packages/2a/21/f691fb2613100a62b3fa91e9988c991e9ca5b89ea31c0d3152a3210344f9/rank_bm25-0.2.2-py3-none-any.whl.metadata\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rank-bm25) (1.26.0)\n",
      "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Installing collected packages: rank-bm25\n",
      "Successfully installed rank-bm25-0.2.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install rank-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import transformers\n",
    "from ast import literal_eval\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import evaluate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq, DataCollatorForLanguageModeling,DataCollatorWithPadding\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from rank_bm25 import BM25Okapi\n",
    "import random\n",
    "\n",
    "\n",
    "# 1. ì„¤ì •: pandas ì¶œë ¥ ì˜µì…˜ ë° ì‹œë“œ ê³ ì •\n",
    "pd.set_option('display.max_columns', None)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1cacacc53954428b82cda27f83cc1c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "model_name = \"./fine_tuned_llama8B_model\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# 3. PEFT ì„¤ì • (LoRA)\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=['q_proj', 'v_proj'],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •\n",
    "model.eval()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "dataset_path = '/data/data/test.csv'  # í•„ìš”í•œ ê²½ë¡œë¡œ ì—…ë°ì´íŠ¸í•˜ì„¸ìš”\n",
    "dataset = pd.read_csv(dataset_path)\n",
    "\n",
    "# JSON í˜•íƒœë¡œ ì €ì¥ëœ 'problems' ì»¬ëŸ¼ì„ í¼ì¹˜ê¸°\n",
    "records = []\n",
    "for _, row in dataset.iterrows():\n",
    "    problems = literal_eval(row['problems'])\n",
    "    record = {\n",
    "        'id': row['id'],\n",
    "        'paragraph': row['paragraph'],\n",
    "        'question': problems['question'],\n",
    "        'choices': problems['choices'],\n",
    "        \"question_plus\": problems.get('question_plus', None),\n",
    "    }\n",
    "    records.append(record)\n",
    "\n",
    "df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜\n",
    "PROMPT_TEMPLATE = \"\"\"ë‹¤ìŒ ì§€ë¬¸ì„ ì½ê³  ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.\n",
    "\n",
    "ì§€ë¬¸:\n",
    "{paragraph}\n",
    "\n",
    "ì§ˆë¬¸:\n",
    "{question}\n",
    "\n",
    "{question_plus}\n",
    "ì„ íƒì§€:\n",
    "{choices}\n",
    "\n",
    "ì •ë‹µì„ ì„ íƒì§€ ì¤‘ì—ì„œ ì„ íƒí•˜ì—¬ ì‘ì„±í•˜ì„¸ìš”:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = []\n",
    "for i in range(len(df)):\n",
    "    choices = df.iloc[i][\"choices\"]\n",
    "    choices_string = \"\\n\".join([f\"{idx + 1}. {choice}\" for idx, choice in enumerate(choices)])\n",
    "    paragraph = df.iloc[i][\"paragraph\"]\n",
    "    question = df.iloc[i][\"question\"]\n",
    "    question_plus = df.iloc[i].get(\"question_plus\", None)\n",
    "\n",
    "    # ì…ë ¥ ë°ì´í„° ìƒì„±\n",
    "    input_text = PROMPT_TEMPLATE.format(\n",
    "        paragraph=paragraph,\n",
    "        question=question,\n",
    "        question_plus=f\"<ë³´ê¸°>:\\n{question_plus}\" if question_plus else \"\",\n",
    "        choices=choices_string,\n",
    "    )\n",
    "\n",
    "    processed_dataset.append(\n",
    "        {\n",
    "            \"id\": df.iloc[i][\"id\"],\n",
    "            \"input\": input_text,\n",
    "            \"choices\": choices,\n",
    "        }\n",
    "    )\n",
    "\n",
    "processed_df = pd.DataFrame(processed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°°ì¹˜ ë‹¨ìœ„ ì„¤ì •\n",
    "batch_size = 1  # ì›í•˜ëŠ” ë°°ì¹˜ í¬ê¸°ë¡œ ì„¤ì •\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ìƒì„± í•¨ìˆ˜ ì •ì˜ (ë°°ì¹˜ ì²˜ë¦¬)\n",
    "def generate_answers(input_texts):\n",
    "    inputs = tokenizer(\n",
    "        input_texts,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=1024,\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # ë‹µë³€ ìƒì„±\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=1024 + 512,  # ì…ë ¥ ê¸¸ì´ + ì˜ˆìƒ ë‹µë³€ ê¸¸ì´\n",
    "            num_beams=5,\n",
    "            early_stopping=True,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            no_repeat_ngram_size=2,\n",
    "        )\n",
    "\n",
    "    # ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©\n",
    "    generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    # ì…ë ¥ í…ìŠ¤íŠ¸ ë¶€ë¶„ì„ ì œì™¸í•˜ê³  ë‹µë³€ë§Œ ì¶”ì¶œ\n",
    "    generated_answers = []\n",
    "    for input_text, generated_text in zip(input_texts, generated_texts):\n",
    "        answer = generated_text[len(input_text):].strip()\n",
    "        generated_answers.append(answer)\n",
    "\n",
    "    return generated_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìœ ì‚¬ë„ ê³„ì‚° ë° ê°€ì¥ ìœ ì‚¬í•œ ì„ íƒì§€ ì„ íƒ í•¨ìˆ˜ ì •ì˜\n",
    "def select_best_choices(generated_answers, choices_list):\n",
    "    selected_indices = []\n",
    "    for generated_answer, choices in zip(generated_answers, choices_list):\n",
    "        # ì„ íƒì§€ì™€ ìƒì„±ëœ ë‹µë³€ì„ í† í°í™”\n",
    "        tokenized_choices = [tokenizer.tokenize(choice) for choice in choices]\n",
    "        tokenized_answer = tokenizer.tokenize(generated_answer)\n",
    "\n",
    "        # BM25 ê°ì²´ ìƒì„±\n",
    "        bm25 = BM25Okapi(tokenized_choices)\n",
    "\n",
    "        # ìœ ì‚¬ë„ ìŠ¤ì½”ì–´ ê³„ì‚°\n",
    "        scores = bm25.get_scores(tokenized_answer)\n",
    "\n",
    "        # ê°€ì¥ ë†’ì€ ìŠ¤ì½”ì–´ì˜ ì„ íƒì§€ ì„ íƒ\n",
    "        best_choice_index = int(np.argmax(scores))\n",
    "        selected_indices.append(best_choice_index + 1)  # ì„ íƒì§€ëŠ” 1ë¶€í„° ì‹œì‘í•˜ë¯€ë¡œ +1\n",
    "\n",
    "    return selected_indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì „ì²´ ì¶”ë¡  í›„ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/109 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  1%|          | 1/109 [01:35<2:52:01, 95.57s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  2%|â–         | 2/109 [03:10<2:50:05, 95.38s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  3%|â–         | 3/109 [04:47<2:49:43, 96.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  4%|â–         | 4/109 [06:24<2:48:39, 96.37s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  5%|â–         | 5/109 [08:00<2:47:05, 96.40s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  6%|â–Œ         | 6/109 [11:48<4:02:11, 141.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "  6%|â–‹         | 7/109 [13:24<3:34:32, 126.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  7%|â–‹         | 8/109 [15:00<3:16:23, 116.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  8%|â–Š         | 9/109 [16:36<3:03:39, 110.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  9%|â–‰         | 10/109 [18:12<2:54:36, 105.82s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 10%|â–ˆ         | 11/109 [19:49<2:48:21, 103.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 11%|â–ˆ         | 12/109 [24:24<4:11:00, 155.26s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 12%|â–ˆâ–        | 13/109 [28:58<5:05:58, 191.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 13%|â–ˆâ–        | 14/109 [30:34<4:17:39, 162.73s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 14%|â–ˆâ–        | 15/109 [32:10<3:43:11, 142.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 15%|â–ˆâ–        | 16/109 [33:47<3:19:43, 128.86s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 16%|â–ˆâ–Œ        | 17/109 [35:24<3:02:53, 119.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 17%|â–ˆâ–‹        | 18/109 [37:00<2:50:22, 112.33s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 17%|â–ˆâ–‹        | 19/109 [41:17<3:53:30, 155.67s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 18%|â–ˆâ–Š        | 20/109 [45:43<4:39:56, 188.72s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 19%|â–ˆâ–‰        | 21/109 [49:27<4:52:38, 199.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 20%|â–ˆâ–ˆ        | 22/109 [51:03<4:04:04, 168.33s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 21%|â–ˆâ–ˆ        | 23/109 [52:39<3:29:55, 146.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 22%|â–ˆâ–ˆâ–       | 24/109 [54:15<3:06:24, 131.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 23%|â–ˆâ–ˆâ–       | 25/109 [55:53<2:49:52, 121.33s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 24%|â–ˆâ–ˆâ–       | 26/109 [57:30<2:37:40, 113.98s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 25%|â–ˆâ–ˆâ–       | 27/109 [1:00:06<2:53:15, 126.78s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 26%|â–ˆâ–ˆâ–Œ       | 28/109 [1:04:34<3:48:23, 169.18s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 27%|â–ˆâ–ˆâ–‹       | 29/109 [1:09:03<4:25:30, 199.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 28%|â–ˆâ–ˆâ–Š       | 30/109 [1:13:41<4:53:01, 222.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 28%|â–ˆâ–ˆâ–Š       | 31/109 [1:18:24<5:12:55, 240.71s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 29%|â–ˆâ–ˆâ–‰       | 32/109 [1:22:47<5:17:33, 247.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 33/109 [1:27:29<5:26:32, 257.80s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 31%|â–ˆâ–ˆâ–ˆ       | 34/109 [1:31:45<5:21:44, 257.39s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 35/109 [1:35:41<5:09:21, 250.83s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–      | 36/109 [1:40:27<5:18:05, 261.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 37/109 [1:45:10<5:21:32, 267.95s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–      | 38/109 [1:49:49<5:20:54, 271.19s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 39/109 [1:52:51<4:45:23, 244.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 40/109 [1:56:20<4:28:47, 233.73s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 41/109 [2:00:27<4:29:37, 237.90s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–Š      | 42/109 [2:04:56<4:35:47, 246.98s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–‰      | 43/109 [2:09:20<4:37:22, 252.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 44/109 [2:13:19<4:28:49, 248.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 45/109 [2:17:57<4:34:31, 257.37s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 46/109 [2:19:33<3:39:14, 208.81s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 47/109 [2:21:10<3:01:00, 175.17s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 48/109 [2:22:47<2:34:27, 151.92s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 49/109 [2:24:24<2:15:22, 135.38s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 50/109 [2:26:00<2:01:23, 123.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 51/109 [2:30:35<2:43:26, 169.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 52/109 [2:35:17<3:12:42, 202.84s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 53/109 [2:40:04<3:32:57, 228.16s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 54/109 [2:42:32<3:07:07, 204.14s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 55/109 [2:44:10<2:34:58, 172.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 56/109 [2:45:48<2:12:23, 149.88s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 57/109 [2:47:25<1:56:12, 134.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 58/109 [2:49:02<1:44:25, 122.85s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 59/109 [2:50:38<1:35:48, 114.97s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 60/109 [2:52:15<1:29:34, 109.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 61/109 [2:53:53<1:24:45, 105.95s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 62/109 [2:57:24<1:47:45, 137.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 63/109 [3:00:00<1:49:36, 142.96s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 64/109 [3:01:42<1:38:03, 130.74s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 65/109 [3:03:19<1:28:32, 120.73s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 66/109 [3:04:57<1:21:29, 113.71s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 67/109 [3:06:58<1:21:11, 116.00s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 68/109 [3:08:35<1:15:28, 110.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 69/109 [3:10:13<1:11:07, 106.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 70/109 [3:12:26<1:14:29, 114.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 71/109 [3:14:56<1:19:18, 125.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 72/109 [3:16:34<1:12:05, 116.90s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 73/109 [3:18:11<1:06:32, 110.89s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 74/109 [3:19:48<1:02:16, 106.75s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 75/109 [3:21:26<58:58, 104.06s/it]  Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 76/109 [3:23:02<56:01, 101.87s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 77/109 [3:24:40<53:40, 100.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 78/109 [3:26:17<51:29, 99.65s/it] Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 79/109 [3:27:56<49:36, 99.21s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 80/109 [3:31:40<1:06:02, 136.62s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 81/109 [3:35:12<1:14:24, 159.43s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 82/109 [3:36:49<1:03:20, 140.77s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 83/109 [3:40:36<1:12:07, 166.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 84/109 [3:42:50<1:05:19, 156.78s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 85/109 [3:44:27<55:32, 138.84s/it]  Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 86/109 [3:46:04<48:25, 126.31s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 87/109 [3:47:41<43:05, 117.55s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 88/109 [3:50:40<47:35, 135.96s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 89/109 [3:52:17<41:24, 124.23s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 90/109 [3:53:54<36:47, 116.16s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 91/109 [3:57:26<43:26, 144.79s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 92/109 [3:59:58<41:41, 147.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 93/109 [4:02:09<37:55, 142.19s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 94/109 [4:03:46<32:09, 128.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 95/109 [4:05:24<27:52, 119.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 96/109 [4:07:14<25:14, 116.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 97/109 [4:09:30<24:29, 122.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 98/109 [4:11:07<21:03, 114.83s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 99/109 [4:12:45<18:15, 109.58s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 100/109 [4:14:22<15:54, 106.02s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 101/109 [4:15:59<13:46, 103.31s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 102/109 [4:17:37<11:51, 101.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 103/109 [4:19:14<10:02, 100.38s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 104/109 [4:20:55<08:22, 100.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 105/109 [4:22:36<06:41, 100.46s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 106/109 [4:24:13<04:58, 99.57s/it] Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 107/109 [4:25:50<03:17, 98.89s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 108/109 [4:27:28<01:38, 98.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 109/109 [4:29:50<00:00, 148.54s/it]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "# ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„° ì²˜ë¦¬\n",
    "for idx in tqdm(range(0, len(processed_df), batch_size)):\n",
    "    batch = processed_df.iloc[idx:idx+batch_size]\n",
    "    input_texts = batch['input'].tolist()\n",
    "    choices_list = batch['choices'].tolist()\n",
    "    ids = batch['id'].tolist()\n",
    "\n",
    "    # ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„±\n",
    "    generated_answers = generate_answers(input_texts)\n",
    "\n",
    "    # ê°€ì¥ ìœ ì‚¬í•œ ì„ íƒì§€ ì„ íƒ\n",
    "    answer_indices = select_best_choices(generated_answers, choices_list)\n",
    "\n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    for id_, answer_index in zip(ids, answer_indices):\n",
    "        results.append({\n",
    "            'id': id_,\n",
    "            'answer': answer_index,\n",
    "        })\n",
    "\n",
    "# ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# CSVë¡œ ì €ì¥\n",
    "output_path = 'output.csv'  # í•„ìš”í•œ ê²½ë¡œë¡œ ì—…ë°ì´íŠ¸í•˜ì„¸ìš”\n",
    "results_df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### í•œí–‰ë§Œ ì¶”ë¡ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     id  answer\n",
      "0  generation-for-nlp-0       4\n",
      "[\"ë‹¤ì‹œ í•œ ë²ˆ ì½ì–´ë³´ì„¸ìš”. ìœ— ê¸€ì—ì„œ ê°€ì¥ ì¤‘ìš”í•˜ê²Œ ê°•ì¡°í•œ ë‚´ìš©ì€ 'ë…ì'ì™€ 'ì±…'ì˜ ê´€ê³„ì— ê´€í•œ ê²ƒì´ì—ˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ, ì´ì™€ ê´€ë ¨ì´ ì—†ëŠ” ì„ íƒì§€ë¥¼ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤. ì„ íƒì§€ëŠ” ì´ ë‹¤ì„¯ ê°€ì§€ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì²« ë²ˆì§¸ëŠ” 'ê°™ì€ ì±…'ì— ëŒ€í•œ ë‚´ìš©ì…ë‹ˆë‹¤. ë‘ ë²ˆì§¸ì™€ ì„¸ ë²ˆì§¸, ê·¸ë¦¬ê³  ë„¤ ë²ˆì§¸ê°€ ëª¨ë‘ ë…ìë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í•œ ë‚´ìš©ì´ê¸° ë•Œë¬¸ì—, ê°€ì¥ ì í•©í•œ ì„ íƒì§€ê°€ ë  ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤. ë‹¤ë§Œ, 'ì½ê¸°' í™˜ê²½ê³¼ 'ê³¼ì œ'ê°€ 'ë°°ê²½' ì§€ì‹ì„ 'ê´€ì 'ë³´ë‹¤ ëœ ì¤‘ìš”í•˜ë‹¤ê³  í•  ìˆ˜ ìˆì§€ë§Œ, ì—¬ì „íˆ ë…ìê¸°ëŠ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ìš”ì†Œë“¤ë¡œ ê°„ì£¼ë©ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, ì±… ì†ì˜ 'ì‚¬íšŒ'ë‚˜ 'ì‹œëŒ€'ê³¼ í•„ìì˜ 'ëŒ€í™”'ë¥¼ ì¤‘ì ì ìœ¼ë¡œ ë‹¤ë£¨ê³  ìˆìœ¼ë¯€ë¡œ '5'ë¼ëŠ” ë²ˆí˜¸ë¥¼ ì„ íƒí•˜ëŠ” ê²ƒì´ ê°€ì¥ í•©ë¦¬ì ì¸ ì„ íƒì¼ ê²ƒì…ë‹ˆë‹¤.\"]\n"
     ]
    }
   ],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹: ì²« ë²ˆì§¸ í–‰ë§Œ ì„ íƒ\n",
    "test_batch = processed_df.iloc[:1]  # ì²« ë²ˆì§¸ í–‰ ì„ íƒ\n",
    "input_texts = test_batch['input'].tolist()\n",
    "choices_list = test_batch['choices'].tolist()\n",
    "ids = test_batch['id'].tolist()\n",
    "\n",
    "# ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„±\n",
    "generated_answers = generate_answers(input_texts)\n",
    "\n",
    "# ê°€ì¥ ìœ ì‚¬í•œ ì„ íƒì§€ ì„ íƒ\n",
    "answer_indices = select_best_choices(generated_answers, choices_list)\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "results = []\n",
    "for id_, answer_index in zip(ids, answer_indices):\n",
    "    results.append({\n",
    "        'id': id_,\n",
    "        'answer': answer_index,\n",
    "    })\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "print(generated_answers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
