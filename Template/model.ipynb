{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import transformers\n",
    "from ast import literal_eval\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM, SFTConfig\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq, DataCollatorForLanguageModeling\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# 1. 설정: pandas 출력 옵션 및 시드 고정\n",
    "pd.set_option('display.max_columns', None)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "set_seed(42)\n",
    "# 2. 모델 및 토크나이저 로드 (8-bit 양자화)\n",
    "model_name = \"CarrotAI/Llama-3.2-Rabbit-Ko-1B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # QLoRA는 4bit 양자화를 사용\n",
    "    # load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # 계산 precision (float16 또는 bfloat16 사용 가능)\n",
    "    bnb_4bit_use_double_quant=True,       # 이중 양자화 활성화\n",
    "    bnb_4bit_quant_type=\"nf4\"             # NF4 양자화 타입\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,  # BitsAndBytesConfig 추가\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"  \n",
    "\n",
    "\n",
    "# 3. PEFT 설정 (LoRA)\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=['q_proj', 'v_proj', 'k_proj', 'o_proj'],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(dataset, prompts, system_prompt):\n",
    "    records = []\n",
    "    for _, row in dataset.iterrows():\n",
    "        problems = literal_eval(row['problems'])\n",
    "        record = {\n",
    "            'id': row['id'],\n",
    "            'paragraph': row['paragraph'],\n",
    "            'question': problems['question'],\n",
    "            'choices': problems['choices'],\n",
    "            'answer': problems.get('answer', None),\n",
    "            \"question_plus\": problems.get('question_plus', ''),\n",
    "            'klue' : row.get('klue', None),\n",
    "            'question_type' : row.get('question_type', None)\n",
    "        }\n",
    "        records.append(record)\n",
    "            \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(records)\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    processed_dataset = []\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        choices_string = \"\\n\".join([f\"{idx + 1} - {choice}\" for idx, choice in enumerate(dataset[i][\"choices\"])])\n",
    "        if dataset[i]['question_type'] == '이해형':\n",
    "            prompt = prompts.이해형\n",
    "        elif dataset[i]['question_type'] == '기타':\n",
    "            prompt = prompts.기타\n",
    "        elif dataset[i]['question_type'] == '사실형':\n",
    "            prompt = prompts.사실형\n",
    "        elif dataset[i]['question_type'] == '추론형':\n",
    "            prompt = prompts.추론형\n",
    "        else:\n",
    "            prompt = prompts.나열형\n",
    "        user_message = prompt.format(\n",
    "            paragraph=dataset[i][\"paragraph\"],\n",
    "            question=dataset[i][\"question\"],\n",
    "            question_plus=dataset[i][\"question_plus\"],\n",
    "            question_type=dataset[i]['question_type'],\n",
    "            choices=choices_string,\n",
    "        )\n",
    "        # chat message 형식으로 변환\n",
    "        processed_dataset.append(\n",
    "            {\n",
    "                \"id\": dataset[i][\"id\"],\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_message},\n",
    "                    {\"role\": \"assistant\", \"content\": f\"{dataset[i]['klue']}\"}\n",
    "                ],\n",
    "                \"label\": dataset[i]['answer'],\n",
    "                'type' : dataset[i]['question_type']\n",
    "            }\n",
    "        )\n",
    "    processed_dataset = Dataset.from_pandas(pd.DataFrame(processed_dataset))\n",
    "\n",
    "    def formatting_prompts_func(example):\n",
    "        output_texts = []\n",
    "        for i in range(len(example[\"messages\"])):\n",
    "            output_texts.append(\n",
    "                tokenizer.apply_chat_template(\n",
    "                    example[\"messages\"][i],\n",
    "                    tokenize=False,\n",
    "                )\n",
    "            )\n",
    "        return output_texts\n",
    "\n",
    "    def tokenize(element):\n",
    "        outputs = tokenizer(\n",
    "            formatting_prompts_func(element),\n",
    "            truncation=False,\n",
    "            padding=False,\n",
    "            return_overflowing_tokens=False,\n",
    "            return_length=False,\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": outputs[\"input_ids\"],\n",
    "            \"attention_mask\": outputs[\"attention_mask\"],\n",
    "        }\n",
    "\n",
    "    # 데이터 토큰화\n",
    "    tokenized_dataset = processed_dataset.map(\n",
    "        tokenize,\n",
    "\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        load_from_cache_file=True,\n",
    "        desc=\"Tokenizing\",\n",
    "    )\n",
    "\n",
    "    # 데이터 분리\n",
    "    tokenized_dataset = tokenized_dataset.filter(lambda x: len(x[\"input_ids\"]) <= 2048)  \n",
    "    tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.10, seed=42)\n",
    "\n",
    "    train_dataset = tokenized_dataset['train']\n",
    "    eval_dataset = tokenized_dataset['test']\n",
    "\n",
    "\n",
    "    train_dataset_token_lengths = [len(train_dataset[i][\"input_ids\"]) for i in range(len(train_dataset))]\n",
    "    print(f\"max token length: {max(train_dataset_token_lengths)}\")\n",
    "    print(f\"min token length: {min(train_dataset_token_lengths)}\")\n",
    "    print(f\"avg token length: {np.mean(train_dataset_token_lengths)}\")\n",
    "\n",
    "    # 데이터 확인\n",
    "    return train_dataset, eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 2031/2031 [00:03<00:00, 520.46 examples/s]\n",
      "Filter: 100%|██████████| 2031/2031 [00:01<00:00, 1441.06 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max token length: 1664\n",
      "min token length: 214\n",
      "avg token length: 750.4258347016968\n"
     ]
    }
   ],
   "source": [
    "from prompts import user_prompts\n",
    "system_prompt = \"\"\"지시에 따라 주어진 문제의 정답을 구하세요.\"\"\"\n",
    "prompts = user_prompts\n",
    "dataset = pd.read_csv('datas/train+klue.csv')\n",
    "train_dataset, eval_dataset = make_dataset(dataset,prompts, system_prompt)\n",
    "# 여기서 프롬프트만 바꿔서 데이터셋을 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "지시에 따라 주어진 문제의 정답을 구하세요.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "문제 유형:\n",
      "기타\n",
      "\n",
      "지문:\n",
      "본문 1:\n",
      "“전하, 게다가 우리들의 왕국에는 신께 도움이 되지 않는 불편함이 존재합니다. 바로 우리 백성들 중 많은 이들이 그대의 백성들이 가져오는 왕국의 상품과 물건을 간절히 원하고 있다는 것, 그런데 그대의 백성들은 자신들의 탐욕스러운 욕망을 만족시키기 위해 자유민이자 해방된 나의 백성들을 잡아가고 있다는 것입니다. 심지어 귀족과 왕의 친척까지도 잡아가 우리들의 왕국에 있는 백인들에게 팔고 있다는 것입니다.”\n",
      "콩고의 아폰소 1세 국왕이 포르투갈의 주앙 3세 국왕에게 보낸 편지, 1526\n",
      "출처 2:\n",
      "“이번 원정에 많은 비용이 들었기에 빈 손으로 돌아간다면 합리적이지 못한 일이 될 것이다. 우리의 [주된] 바람은 신을 섬기는 것과 콩고 국왕을 기쁘게 하는 것이지만, 그럼에도 불구하고 콩고 국왕으로 하여금 노예가 됐건 구리가 됐건 상아가 됐건 배를 채워야 한다는 사실을 우리들의 이름으로 이해시켜야만 한다.”\n",
      "포르투갈 마누엘 국왕의 콩고에 있는 사절에게 보낸 편지, 1512\n",
      "\n",
      "질문:\n",
      "편지에 설명된 상호 작용은 다음 중 어떤 맥락에서 가장 잘 이해되는가?\n",
      "\n",
      "선택지:\n",
      "1 - 포르투갈의 서아프리카 해안 탐험\n",
      "2 - 사하라 이남 아프리카에서의 가톨릭 선교 활동\n",
      "3 - 사하라 이남 아프리카의 국가 형성\n",
      "4 - 사하라 이남 아프리카의 노예 무역 발전\n",
      "\n",
      "출력 형식:\n",
      "근거: 답변을 도출한 텍스트 # 정답 번호\n",
      "\n",
      "문제는 무조건 1개의 정답만 있습니다.\n",
      "문제를 풀이할 때, 반드시 지문을 참고하세요.\n",
      "반드시 지문에서 정답의 근거를 찾으세요.\n",
      "반드시 출력 형식을 지키세요.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "근거 : 내용 형식입니다. 본문에서는 콩고 왕국이 포르투갈에 의해 불법적인 인신매매와 탐욕으로 고통받고 있음을 강조하고, 이에 따른 불만이 표현되고 있습니다. 이를 통해 두 왕국 간의 경제적 착취와 권력의 불균형이 드러나며, 포르투갈이 자원을 착취하기 위한 맥락이 설명됩니다. # 4<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(train_dataset['input_ids'][5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    logits = logits if not isinstance(logits, tuple) else logits[0]\n",
    "    logit_idx = [tokenizer.vocab[\"1\"],\n",
    "                    tokenizer.vocab[\"2\"],\n",
    "                    tokenizer.vocab[\"3\"],\n",
    "                    tokenizer.vocab[\"4\"], \n",
    "                    tokenizer.vocab[\"5\"]]\n",
    "    logits = logits[:, -2, logit_idx] # -2: answer token, -1: eos token\n",
    "    return logits\n",
    "\n",
    "\n",
    "    # metric 계산 함수\n",
    "def compute_metrics(evaluation_result):\n",
    "    logits, labels = evaluation_result\n",
    "    int_output_map = {\"1\": 0, \"2\": 1, \"3\": 2, \"4\": 3, \"5\": 4}\n",
    "\n",
    "\n",
    "    # 토큰화된 레이블 디코딩\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    print(labels)\n",
    "    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    labels = [label.split('#')[-1].strip() for label in labels]\n",
    "    labels = [int_output_map.get(label, -1) for label in labels] \n",
    "\n",
    "    # 소프트맥스 함수를 사용하여 로그트 변환\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(logits, dtype=torch.float32), dim=-1)\n",
    "\n",
    "    predictions = np.argmax(probs, axis=-1)\n",
    "\n",
    "    # 정확도 계산\n",
    "    acc = acc_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "\n",
    "    return {\"accuracy\": acc[\"accuracy\"], \"f1\": f1[\"f1\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:463: UserWarning: You passed a dataset that is already processed (contains an `input_ids` field) together with a valid formatting function. Therefore `formatting_func` will be ignored.\n",
      "  warnings.warn(\n",
      "WARNING:accelerate.utils.other:Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "response_template = \"assistant<|end_header_id|>\"\n",
    "data_collator = DataCollatorForCompletionOnlyLM(\n",
    "    response_template=response_template,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_seq_length = 2048,\n",
    "    output_dir=f\"./outputs + {model_name.split('/')[-1]}\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    save_only_model=True,\n",
    "    report_to=\"none\",\n",
    "    fp16 = True,\n",
    "    gradient_checkpointing = False, # 8B모델 돌릴때만 True로, 아니면 False로\n",
    "    load_best_model_at_end = True\n",
    ")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics = preprocess_logits_for_metrics,\n",
    "    args=sft_config,\n",
    "    packing=False,\n",
    "    peft_config = lora_config\n",
    ")\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='228' max='228' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [228/228 07:56, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.430800</td>\n",
       "      <td>1.224366</td>\n",
       "      <td>0.421569</td>\n",
       "      <td>0.136430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=228, training_loss=1.3938894690128796, metrics={'train_runtime': 478.0686, 'train_samples_per_second': 3.822, 'train_steps_per_second': 0.477, 'total_flos': 8007966058217472.0, 'train_loss': 1.3938894690128796, 'epoch': 0.9983579638752053})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from datasets import Dataset\n",
    "\n",
    "def make_test_dataset(test_df, prompt, system_prompt):\n",
    "    # Flatten the JSON dataset\n",
    "    records = []\n",
    "    for _, row in test_df.iterrows():\n",
    "        problems = literal_eval(row['problems'])\n",
    "        record = {\n",
    "            'id': row['id'],\n",
    "            'paragraph': row['paragraph'],\n",
    "            'question': problems['question'],\n",
    "            'choices': problems['choices'],\n",
    "            'answer': problems.get('answer', None),\n",
    "            \"question_plus\": problems.get('question_plus', ''),\n",
    "            'klue': row.get('klue', None),\n",
    "            'question_type': row.get('question_type', None)\n",
    "        }\n",
    "        records.append(record)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(records)\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    \n",
    "    # test_dataset를 빈 리스트로 초기화\n",
    "    test_dataset = []\n",
    "\n",
    "    # dataset의 각 항목에 대해 처리\n",
    "    for i in range(len(dataset)):\n",
    "        # choices_string 생성\n",
    "        choices_string = \"\\n\".join([f\"{idx + 1} - {choice}\" for idx, choice in enumerate(dataset[i][\"choices\"])])\n",
    "        \n",
    "        # question_type에 맞는 prompt 선택\n",
    "        if dataset[i]['question_type'] == '이해형':\n",
    "            prompt = prompts.이해형\n",
    "        elif dataset[i]['question_type'] == '기타':\n",
    "            prompt = prompts.기타\n",
    "        elif dataset[i]['question_type'] == '사실형':\n",
    "            prompt = prompts.사실형\n",
    "        elif dataset[i]['question_type'] == '추론형':\n",
    "            prompt = prompts.추론형\n",
    "        else:\n",
    "            prompt = prompts.나열형\n",
    "\n",
    "        # user_message 생성\n",
    "        user_message = prompt.format(\n",
    "            paragraph=dataset[i][\"paragraph\"],\n",
    "            question=dataset[i][\"question\"],\n",
    "            question_plus=dataset[i][\"question_plus\"],\n",
    "            question_type=dataset[i]['question_type'],\n",
    "            choices=choices_string,\n",
    "        )\n",
    "\n",
    "        # len_choices는 choices의 길이로 계산\n",
    "        len_choices = len(dataset[i][\"choices\"])\n",
    "\n",
    "        # test_dataset에 항목 추가\n",
    "        test_dataset.append(\n",
    "            {\n",
    "                \"id\": dataset[i][\"id\"],  # dataset[i]에서 'id' 가져오기\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_message},\n",
    "                ],\n",
    "                \"label\": dataset[i].get('answer', None),  # 'answer'가 없으면 None\n",
    "                'type': dataset[i].get('question_type', None),\n",
    "                \"len_choices\": len_choices,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return test_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test dataset inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_df = pd.read_csv('datas/test_processing_4.csv')\n",
    "test_dataset = make_test_dataset(test_df, prompt, system_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'generation-for-nlp-785',\n",
       " 'messages': [{'role': 'system', 'content': '지시에 따라 주어진 문제의 정답을 구하세요.'},\n",
       "  {'role': 'user',\n",
       "   'content': '\\n문제 유형:\\n이해형\\n\\n지문:\\nI. MR = P = 완전 경쟁의 수평 함수에 대한 수요. II. 불완전 경쟁의 우하향 함수로서 P > MR. III. 수요와 가격이 불완전 경쟁의 수직 함수로 표시됨.\\n\\n질문:\\n다음 중 완전 경쟁 하에서 가격(P)이 한계 수익(MR)과 같은 이유와 독점 또는 불완전 경쟁 하에서 가격(P)이 한계 수익보다 큰 이유를 올바르게 설명하는 것은 무엇입니까?\\n\\n선택지:\\n1 - I, II 및 III\\n2 - I 및 III\\n3 - I만 해당.\\n4 - I 및 III\\n\\n출력 형식:\\n근거: 답변을 도출한 텍스트 # 정답 번호\\n\\n문제는 무조건 1개의 정답만 있습니다.\\n반드시 지문을 이해하고 이해한 내용을 바탕으로 정답을 고르세요.\\n반드시 출력 형식을 지키세요.\\n'}],\n",
       " 'label': '',\n",
       " 'type': '이해형',\n",
       " 'len_choices': 4}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "100%|██████████| 1/1 [00:12<00:00, 12.68s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "\n",
    "# 배치 데이터 로더를 위한 collate_fn\n",
    "def collate_fn(batch):\n",
    "    ids = [item[\"id\"] for item in batch]\n",
    "    messages = [item[\"messages\"] for item in batch]\n",
    "    labels = [item.get(\"label\", None) for item in batch]  # 라벨이 존재할 경우만 가져옴\n",
    "    return ids, messages, labels\n",
    "\n",
    "# 데이터 로더 설정\n",
    "batch_size = 8  # 배치 크기 설정\n",
    "dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "generated_infer_results = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for batch in tqdm(dataloader):\n",
    "        ids, messages, labels = batch\n",
    "\n",
    "        # 텍스트 생성을 위한 입력 데이터 준비\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,  # 배치 크기에 맞게 패딩 추가\n",
    "        )\n",
    "\n",
    "        # GPU로 이동 (inputs가 Tensor일 경우 바로 이동)\n",
    "        inputs = inputs.to(model.device)  # Tensor로 직접 처리\n",
    "\n",
    "        # 모델 생성\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_new_tokens=150,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,  # 종료 토큰 설정\n",
    "        )\n",
    "\n",
    "        # 결과 디코딩\n",
    "        generated_texts = tokenizer.batch_decode(\n",
    "            outputs[:, inputs.shape[1]:], skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        # 결과 저장\n",
    "        for _id, generated_text, label in zip(ids, generated_texts, labels):\n",
    "            generated_infer_results.append({\n",
    "                \"id\": _id,\n",
    "                \"answer\": generated_text,\n",
    "                \"label\": label  # 실제 라벨이 있다면 포함\n",
    "            })\n",
    "\n",
    "# 결과를 DataFrame으로 저장\n",
    "generated_infer_results = pd.DataFrame(generated_infer_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>answer</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>generation-for-nlp-0</td>\n",
       "      <td>ette.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>generation-for-nlp-1</td>\n",
       "      <td>속) 내용에 따라 내용이 다르며, 그 내용은 독서의 즐거움을 더 강화한다. 독서의 ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>generation-for-nlp-2</td>\n",
       "      <td>�로(가) 중국에서 비롯된 유서(類書)는 고금의 서적에서 자료를 수집하고 항목별로 ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>generation-for-nlp-3</td>\n",
       "      <td>�지</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>generation-for-nlp-4</td>\n",
       "      <td>�용은 지식의 역학적 진화를 수용하는 내용을 강조하고, ㉡에 대한 이해는 ㉡의 성격...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>generation-for-nlp-5</td>\n",
       "      <td>�문\\n\\n㉡의 말에 따르면, '이수광'의 말은 '중국 유서'가 '중국 유서'의 지...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>generation-for-nlp-6</td>\n",
       "      <td>질문에 대한 답변은 다음과 같습니다.\\n\\n[근거: 답변] # 3 - 당대 지식을 ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>generation-for-nlp-7</td>\n",
       "      <td>�상상상상상상상상상상상상상상상상상상상상상상상상상상상상상상상상상상상상상상상상상상상상상...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                             answer  \\\n",
       "0  generation-for-nlp-0                                              ette.   \n",
       "1  generation-for-nlp-1  속) 내용에 따라 내용이 다르며, 그 내용은 독서의 즐거움을 더 강화한다. 독서의 ...   \n",
       "2  generation-for-nlp-2  �로(가) 중국에서 비롯된 유서(類書)는 고금의 서적에서 자료를 수집하고 항목별로 ...   \n",
       "3  generation-for-nlp-3                                                 �지   \n",
       "4  generation-for-nlp-4  �용은 지식의 역학적 진화를 수용하는 내용을 강조하고, ㉡에 대한 이해는 ㉡의 성격...   \n",
       "5  generation-for-nlp-5  �문\\n\\n㉡의 말에 따르면, '이수광'의 말은 '중국 유서'가 '중국 유서'의 지...   \n",
       "6  generation-for-nlp-6  질문에 대한 답변은 다음과 같습니다.\\n\\n[근거: 답변] # 3 - 당대 지식을 ...   \n",
       "7  generation-for-nlp-7  �상상상상상상상상상상상상상상상상상상상상상상상상상상상상상상상상상상상상상상상상상상상상상...   \n",
       "\n",
       "  label  \n",
       "0        \n",
       "1        \n",
       "2        \n",
       "3        \n",
       "4        \n",
       "5        \n",
       "6        \n",
       "7        "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_infer_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ette.\n"
     ]
    }
   ],
   "source": [
    "print(generated_infer_results.loc[0,'answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'근거: 답변을 도출한 내용에 따르면, (가)의 실학자들의 유서와 (나)의 유서는 모두 주자학의 관념적 사유를 반영한다. (나)의 유서는 주자학의 지적 영역 내에서 서학의 지식을 수용하는 내용을 보여주고, 이는 서학의 진보성의 토대가 중국이라는 서학 중국 원류설을 반영한 것이었다. 이는 (가)와 (나) 모두 서학의 지식의 역사적 의미를 강조한다. # [정답 번호]'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_infer_results['answer'][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_last_digit(s):\n",
    "    match = re.search(r'\\d$', s)  # 문자열 끝에서 숫자 하나만 매칭\n",
    "    return match.group() if match else None  # 매칭된 숫자를 반환, 없으면 None 반환\n",
    "\n",
    "result = generated_infer_results\n",
    "# 데이터프레임에 적용\n",
    "result['text'] = result['answer']\n",
    "result['answer'] = result['answer'].apply(lambda x: x.split('#')[-1])\n",
    "result['answer'] = result['answer'].apply(extract_last_digit)\n",
    "result['answer'] = result['answer'].fillna(1)\n",
    "result['answer'] = result['answer'].apply(lambda x: int(x))\n",
    "submission = result[['id', 'text', 'answer']]\n",
    "submission['answer'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Eval dataset으로 inference\n",
    "#### 프롬프트를 바꿔가며 몇개 맞추나 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [03:17<00:00,  7.59s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.padding_side = 'left'\n",
    "batch_size = 8  # 배치 크기 설정\n",
    "dataloader = DataLoader(eval_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "generated_infer_results = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for batch in tqdm(dataloader):\n",
    "        ids, messages, labels = batch\n",
    "\n",
    "        # 텍스트 생성을 위한 입력 데이터 준비\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,  # 배치 크기에 맞게 패딩 추가\n",
    "        )\n",
    "\n",
    "        # GPU로 이동 (inputs가 Tensor일 경우 바로 이동)\n",
    "        inputs = inputs.to(model.device)  # Tensor로 직접 처리\n",
    "\n",
    "        # 모델 생성\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_new_tokens=150,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,  # 종료 토큰 설정\n",
    "        )\n",
    "\n",
    "        # 결과 디코딩\n",
    "        generated_texts = tokenizer.batch_decode(\n",
    "            outputs[:, inputs.shape[1]:], skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "\n",
    "        # 결과 저장\n",
    "        for _id, generated_text, label in zip(ids, generated_texts, labels):\n",
    "            generated_infer_results.append({\n",
    "                \"id\": _id,\n",
    "                \"answer\": generated_text,\n",
    "                \"label\": label  # 실제 라벨이 있다면 포함\n",
    "            })\n",
    "\n",
    "# 결과를 DataFrame으로 저장\n",
    "generated_infer_results = pd.DataFrame(generated_infer_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import re\n",
    "def extract_last_digit(s):\n",
    "    match = re.search(r'\\d$', s)  # 문자열 끝에서 숫자 하나만 매칭\n",
    "    return match.group() if match else None  # 매칭된 숫자를 반환, 없으면 None 반환\n",
    "\n",
    "result = deepcopy(generated_infer_results)\n",
    "# 데이터프레임에 적용\n",
    "result['text'] = result['answer']\n",
    "result['predict'] = result['answer'].apply(lambda x: x.split('#')[-1])\n",
    "result['predict'] = result['predict'].apply(extract_last_digit)\n",
    "result['predict'] = result['predict'].fillna(1)\n",
    "result['predict'] = result['predict'].apply(lambda x: int(x))\n",
    "\n",
    "result = result[['text', 'predict', 'label',]]\n",
    "result['question_type'] = eval_dataset['type']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>predict</th>\n",
       "      <th>label</th>\n",
       "      <th>question_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>근거 : 내용에 따르면, 로열 베이비 조지 알렉산더 루이스 왕자가 태어난 후 아덴아...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>기타</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>근거 : 내용에 따르면, 문화스포츠계에서 가수 정기고와 박정현의 신곡 발매 계획이 ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>이해형</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>근거 : 위로 기울어지는 총 공급 곡선은 공급의 감소를 나타내며, 이는 균형 물가 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>기타</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>근거 : Andy의 행동은 자기중심주의를 나타내며, 자신의 기대와 목표를 우선시하고...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>기타</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>근거 : 내용에 따르면, 코로나19 감염예방 홍보 가이드에는 마스크 착용 권장 안내...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>이해형</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>근거 : 카프의 어머니는 학교를 중단하는 것을 권장하며, 그의 열정을 살리기 위한 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>이해형</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>근거 : 내용에 따르면, 올랭프 드 구주는 혁명 시대에 여성은 일반적으로 지도자 역...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>이해형</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>근거 : 내용에 따르면, 11월 수출액은 작년 같은 달보다 0.2% 증가했다고 명확...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>기타</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>근거 : 내용 형식으로 보면, 이 지문은 아리스토텔레스와 플라톤의 철학적 우열 논쟁...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>이해형</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>근거 : 내용 형식입니다. 글은 로마 가톨릭 교도들이 세속 권력을 강조하고, 교황의...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>기타</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>204 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  predict  label  \\\n",
       "0    근거 : 내용에 따르면, 로열 베이비 조지 알렉산더 루이스 왕자가 태어난 후 아덴아...        1      1   \n",
       "1    근거 : 내용에 따르면, 문화스포츠계에서 가수 정기고와 박정현의 신곡 발매 계획이 ...        5      1   \n",
       "2    근거 : 위로 기울어지는 총 공급 곡선은 공급의 감소를 나타내며, 이는 균형 물가 ...        1      1   \n",
       "3    근거 : Andy의 행동은 자기중심주의를 나타내며, 자신의 기대와 목표를 우선시하고...        1      4   \n",
       "4    근거 : 내용에 따르면, 코로나19 감염예방 홍보 가이드에는 마스크 착용 권장 안내...        1      1   \n",
       "..                                                 ...      ...    ...   \n",
       "199  근거 : 카프의 어머니는 학교를 중단하는 것을 권장하며, 그의 열정을 살리기 위한 ...        1      1   \n",
       "200  근거 : 내용에 따르면, 올랭프 드 구주는 혁명 시대에 여성은 일반적으로 지도자 역...        3      1   \n",
       "201  근거 : 내용에 따르면, 11월 수출액은 작년 같은 달보다 0.2% 증가했다고 명확...        1      1   \n",
       "202  근거 : 내용 형식으로 보면, 이 지문은 아리스토텔레스와 플라톤의 철학적 우열 논쟁...        1      1   \n",
       "203  근거 : 내용 형식입니다. 글은 로마 가톨릭 교도들이 세속 권력을 강조하고, 교황의...        1      3   \n",
       "\n",
       "    question_type  \n",
       "0              기타  \n",
       "1             이해형  \n",
       "2              기타  \n",
       "3              기타  \n",
       "4             이해형  \n",
       "..            ...  \n",
       "199           이해형  \n",
       "200           이해형  \n",
       "201            기타  \n",
       "202           이해형  \n",
       "203            기타  \n",
       "\n",
       "[204 rows x 4 columns]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
