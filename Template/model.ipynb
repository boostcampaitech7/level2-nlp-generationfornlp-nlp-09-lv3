{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.12s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import transformers\n",
    "from ast import literal_eval\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM, SFTConfig\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq, DataCollatorForLanguageModeling\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# 1. 설정: pandas 출력 옵션 및 시드 고정\n",
    "pd.set_option('display.max_columns', None)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "set_seed(42)\n",
    "# 2. 모델 및 토크나이저 로드 (8-bit 양자화)\n",
    "model_name = \"CarrotAI/Llama-3.2-Rabbit-Ko-3B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # QLoRA는 4bit 양자화를 사용\n",
    "    # load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # 계산 precision (float16 또는 bfloat16 사용 가능)\n",
    "    bnb_4bit_use_double_quant=True,       # 이중 양자화 활성화\n",
    "    bnb_4bit_quant_type=\"nf4\"             # NF4 양자화 타입\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,  # BitsAndBytesConfig 추가\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"  # 왼쪽 패딩 설정\n",
    "\n",
    "\n",
    "# 3. PEFT 설정 (LoRA)\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=['q_proj', 'v_proj', 'k_proj', 'o_proj'],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(prompt, prompt_plus, system_prompt):\n",
    "    dataset = pd.read_csv('datas/train+klue.csv')\n",
    "    records = []\n",
    "    for _, row in dataset.iterrows():\n",
    "        problems = literal_eval(row['problems'])\n",
    "        record = {\n",
    "            'id': row['id'],\n",
    "            'paragraph': row['paragraph'],\n",
    "            'question': problems['question'],\n",
    "            'choices': problems['choices'],\n",
    "            'answer': problems.get('klue', None),\n",
    "            \"question_plus\": problems.get('question_plus', None),\n",
    "            'klue' : row.get('klue', None)\n",
    "        }\n",
    "        # Include 'question_plus' if it exists\n",
    "        if 'question_plus' in problems:\n",
    "            record['question_plus'] = problems['question_plus']\n",
    "        records.append(record)\n",
    "            \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(records)\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    processed_dataset = []\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        choices_string = \"\\n\".join([f\"{idx + 1} - {choice}\" for idx, choice in enumerate(dataset[i][\"choices\"])])\n",
    "\n",
    "        # <보기>가 있을 때\n",
    "        if dataset[i][\"question_plus\"]:\n",
    "            user_message = prompt_plus.format(\n",
    "                paragraph=dataset[i][\"paragraph\"],\n",
    "                question=dataset[i][\"question\"],\n",
    "                question_plus=dataset[i][\"question_plus\"],\n",
    "                choices=choices_string,\n",
    "            )\n",
    "        # <보기>가 없을 때\n",
    "        else:\n",
    "            user_message = prompt.format(\n",
    "                paragraph=dataset[i][\"paragraph\"],\n",
    "                question=dataset[i][\"question\"],\n",
    "                choices=choices_string,\n",
    "            )\n",
    "\n",
    "        # chat message 형식으로 변환\n",
    "        processed_dataset.append(\n",
    "            {\n",
    "                \"id\": dataset[i][\"id\"],\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_message},\n",
    "                    {\"role\": \"assistant\", \"content\": f\"{dataset[i]['klue']}\"}\n",
    "                ],\n",
    "                \"label\": dataset[i]['klue'],\n",
    "            }\n",
    "        )\n",
    "    processed_dataset = Dataset.from_pandas(pd.DataFrame(processed_dataset))\n",
    "\n",
    "    def formatting_prompts_func(example):\n",
    "        output_texts = []\n",
    "        for i in range(len(example[\"messages\"])):\n",
    "            output_texts.append(\n",
    "                tokenizer.apply_chat_template(\n",
    "                    example[\"messages\"][i],\n",
    "                    tokenize=False,\n",
    "                )\n",
    "            )\n",
    "        return output_texts\n",
    "\n",
    "    def tokenize(element):\n",
    "        outputs = tokenizer(\n",
    "            formatting_prompts_func(element),\n",
    "            truncation=False,\n",
    "            padding=False,\n",
    "            return_overflowing_tokens=False,\n",
    "            return_length=False,\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": outputs[\"input_ids\"],\n",
    "            \"attention_mask\": outputs[\"attention_mask\"],\n",
    "        }\n",
    "\n",
    "    # 데이터 토큰화\n",
    "    tokenized_dataset = processed_dataset.map(\n",
    "        tokenize,\n",
    "        remove_columns=list(processed_dataset.features),\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        load_from_cache_file=True,\n",
    "        desc=\"Tokenizing\",\n",
    "    )\n",
    "\n",
    "    # 데이터 분리\n",
    "    tokenized_dataset = tokenized_dataset.filter(lambda x: len(x[\"input_ids\"]) <= 2048)  \n",
    "    tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.10, seed=42)\n",
    "\n",
    "    train_dataset = tokenized_dataset['train']\n",
    "    eval_dataset = tokenized_dataset['test']\n",
    "\n",
    "\n",
    "    train_dataset_token_lengths = [len(train_dataset[i][\"input_ids\"]) for i in range(len(train_dataset))]\n",
    "    print(f\"max token length: {max(train_dataset_token_lengths)}\")\n",
    "    print(f\"min token length: {min(train_dataset_token_lengths)}\")\n",
    "    print(f\"avg token length: {np.mean(train_dataset_token_lengths)}\")\n",
    "\n",
    "    # 데이터 확인\n",
    "    return train_dataset, eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 2031/2031 [00:04<00:00, 499.22 examples/s]\n",
      "Filter: 100%|██████████| 2031/2031 [00:01<00:00, 1508.12 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max token length: 1683\n",
      "min token length: 233\n",
      "avg token length: 765.5440613026819\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"국어 시험 문제를 푸는 대한민국의 고3 수험생으로서 위의 요약을 바탕으로 다음 문제의 답을 구하세요.\"\"\"\n",
    "prompt = \"\"\"\n",
    "지문:\n",
    "{paragraph}\n",
    "\n",
    "질문:\n",
    "{question}\n",
    "\n",
    "선택지:\n",
    "{choices}\n",
    "\n",
    "출력 형식:\n",
    "[근거: 답변을 도출한 텍스트] # [결과: 선택지 번호]\n",
    "\n",
    "문제를 풀이할 때, 반드시 지문을 참고하세요.\n",
    "문제는 무조건 1개의 정답만 있습니다.\n",
    "문제를 풀이할 때 모든 선택지들을 검토하세요.\n",
    "\n",
    "근거:\n",
    "\"\"\"\n",
    "\n",
    "prompt_plus = \"\"\"\n",
    "지문:\n",
    "{paragraph}\n",
    "\n",
    "질문:\n",
    "{question}\n",
    "\n",
    "<보기>:\n",
    "{question_plus}\n",
    "\n",
    "선택지:\n",
    "{choices}\n",
    "\n",
    "출력 형식:\n",
    "[근거: 답변을 도출한 텍스트] # [결과: 선택지 번호]\n",
    "\n",
    "문제를 풀이할 때, 반드시 지문을 참고하세요.\n",
    "문제는 무조건 1개의 정답만 있습니다.\n",
    "문제를 풀이할 때 모든 선택지들을 검토하세요.\n",
    "\n",
    "근거:\n",
    "\"\"\"\n",
    "\n",
    "train_dataset, eval_dataset = make_dataset(prompt, prompt_plus, system_prompt)\n",
    "# 여기서 프롬프트만 바꿔서 데이터셋을 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    logits = logits if not isinstance(logits, tuple) else logits[0]\n",
    "    logit_idx = [tokenizer.vocab[\"1\"],\n",
    "                    tokenizer.vocab[\"2\"],\n",
    "                    tokenizer.vocab[\"3\"],\n",
    "                    tokenizer.vocab[\"4\"], \n",
    "                    tokenizer.vocab[\"5\"]]\n",
    "    logits = logits[:, -2, logit_idx] # -2: answer token, -1: eos token\n",
    "    return logits\n",
    "\n",
    "\n",
    "    # metric 계산 함수\n",
    "def compute_metrics(evaluation_result):\n",
    "    logits, labels = evaluation_result\n",
    "    int_output_map = {\"1\": 0, \"2\": 1, \"3\": 2, \"4\": 3, \"5\": 4}\n",
    "\n",
    "\n",
    "    # 토큰화된 레이블 디코딩\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    labels = [label.split('#')[-1].strip() for label in labels]\n",
    "    labels = [int_output_map.get(label, -1) for label in labels] \n",
    "\n",
    "    # 소프트맥스 함수를 사용하여 로그트 변환\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(logits, dtype=torch.float32), dim=-1)\n",
    "\n",
    "    predictions = np.argmax(probs, axis=-1)\n",
    "\n",
    "    # 정확도 계산\n",
    "    acc = acc_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "\n",
    "    return {\"accuracy\": acc[\"accuracy\"], \"f1\": f1[\"f1\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '0.13.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='684' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 12/684 00:50 < 56:31, 0.20 it/s, Epoch 0.05/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response_template = \"assistant<|end_header_id|>\"\n",
    "data_collator = DataCollatorForCompletionOnlyLM(\n",
    "    response_template=response_template,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_seq_length = 2048,\n",
    "    output_dir=f\"./outputs + {model_name.split('/')[-1]}\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    save_only_model=True,\n",
    "    report_to=\"none\",\n",
    "    fp16 = True,\n",
    "    gradient_checkpointing = True, # 8B모델 돌릴때만 True로, 아니면 False로\n",
    "    load_best_model_at_end = True\n",
    ")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics = preprocess_logits_for_metrics,\n",
    "    args=sft_config,\n",
    "    max_seq_length=2048,\n",
    "    packing=False,\n",
    "    peft_config = lora_config\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_test_dataset(test_df, prompt, prompt_plus, system_prompt):\n",
    "    # Flatten the JSON dataset\n",
    "    records = []\n",
    "    for _, row in test_df.iterrows():\n",
    "        problems = literal_eval(row['problems'])\n",
    "        record = {\n",
    "            'id': row['id'],\n",
    "            'paragraph': row['paragraph'],\n",
    "            'question': problems['question'],\n",
    "            'choices': problems['choices'],\n",
    "            'answer': problems.get('answer', None),\n",
    "            \"question_plus\": problems.get('question_plus', None),\n",
    "            'klue': row.get('hint', None)\n",
    "        }\n",
    "        # Include 'question_plus' if it exists\n",
    "        if 'question_plus' in problems:\n",
    "            record['question_plus'] = problems['question_plus']\n",
    "        records.append(record)\n",
    "            \n",
    "    # Convert to DataFrame\n",
    "    test_df = pd.DataFrame(records)\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    test_dataset = []\n",
    "    for i, row in test_df.iterrows():\n",
    "        choices_string = \"\\n\".join([f\"{idx + 1} - {choice}\" for idx, choice in enumerate(row[\"choices\"])])\n",
    "        len_choices = len(row[\"choices\"])\n",
    "        \n",
    "        # <보기>가 있을 때\n",
    "        if row[\"question_plus\"]:\n",
    "            user_message = prompt_plus.format(\n",
    "                paragraph=row[\"paragraph\"],\n",
    "                question=row[\"question\"],\n",
    "                question_plus=row[\"question_plus\"],\n",
    "                choices=choices_string,\n",
    "            )\n",
    "        # <보기>가 없을 때\n",
    "        else:\n",
    "            user_message = prompt.format(\n",
    "                paragraph=row[\"paragraph\"],\n",
    "                question=row[\"question\"],\n",
    "                choices=choices_string,\n",
    "            )\n",
    "\n",
    "        test_dataset.append(\n",
    "            {\n",
    "                \"id\": row[\"id\"],\n",
    "                \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_message},\n",
    "                ],\n",
    "                \"label\": row[\"answer\"],\n",
    "                \"len_choices\": len_choices,\n",
    "                'klue' : row['klue']\n",
    "            }\n",
    "        )\n",
    "    return test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:23<00:00, 23.84s/it]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"국어 시험 문제를 푸는 대한민국의 고3 수험생으로서 위의 요약을 바탕으로 다음 문제의 답을 구하세요.\"\"\"\n",
    "# ------------------------------------------------------------------------------------------\n",
    "prompt = \"\"\"\n",
    "지문:\n",
    "{paragraph}\n",
    "\n",
    "질문:\n",
    "{question}\n",
    "\n",
    "선택지:\n",
    "{choices}\n",
    "\n",
    "출력 형식:\n",
    "[근거: 답변을 도출한 텍스트] # [결과: 선택지 번호]\n",
    "\n",
    "문제를 풀이할 때, 반드시 지문을 참고하세요.\n",
    "문제는 무조건 1개의 정답만 있습니다.\n",
    "문제를 풀이할 때 모든 선택지들을 검토하세요.\n",
    "\n",
    "근거:\n",
    "\"\"\"\n",
    "# ------------------------------------------------------------------------------------------\n",
    "prompt_plus = \"\"\"\n",
    "지문:\n",
    "{paragraph}\n",
    "\n",
    "질문:\n",
    "{question}\n",
    "\n",
    "<보기>:\n",
    "{question_plus}\n",
    "\n",
    "선택지:\n",
    "{choices}\n",
    "\n",
    "출력 형식:\n",
    "[근거: 답변을 도출한 텍스트] # [결과: 선택지 번호]\n",
    "\n",
    "문제를 풀이할 때, 반드시 지문을 참고하세요.\n",
    "문제는 무조건 1개의 정답만 있습니다.\n",
    "문제를 풀이할 때 모든 선택지들을 검토하세요.\n",
    "\n",
    "근거:\n",
    "\"\"\"\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "test_df = pd.read_csv('datas/test.csv')\n",
    "test_dataset = make_test_dataset(test_df, prompt, prompt_plus, system_prompt)\n",
    "test_dataset = test_dataset[:8]\n",
    "# 배치 데이터 로더를 위한 collate_fn\n",
    "def collate_fn(batch):\n",
    "    ids = [item[\"id\"] for item in batch]\n",
    "    messages = [item[\"messages\"] for item in batch]\n",
    "    labels = [item.get(\"label\", None) for item in batch]  # 라벨이 존재할 경우만 가져옴\n",
    "    return ids, messages, labels\n",
    "\n",
    "# 데이터 로더 설정\n",
    "batch_size = 8  # 배치 크기 설정\n",
    "dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "generated_infer_results = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for batch in tqdm(dataloader):\n",
    "        ids, messages, labels = batch\n",
    "\n",
    "        # 텍스트 생성을 위한 입력 데이터 준비\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,  # 배치 크기에 맞게 패딩 추가\n",
    "        )\n",
    "\n",
    "        # GPU로 이동 (inputs가 Tensor일 경우 바로 이동)\n",
    "        inputs = inputs.to(model.device)  # Tensor로 직접 처리\n",
    "\n",
    "        # 모델 생성\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_new_tokens=150,  # 최대 생성 토큰 수\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "        # 결과 디코딩\n",
    "        generated_texts = tokenizer.batch_decode(\n",
    "            outputs[:, inputs.shape[1]:], skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        # 결과 저장\n",
    "        for _id, generated_text, label in zip(ids, generated_texts, labels):\n",
    "            generated_infer_results.append({\n",
    "                \"id\": _id,\n",
    "                \"answer\": generated_text,\n",
    "                \"label\": label  # 실제 라벨이 있다면 포함\n",
    "            })\n",
    "\n",
    "# 결과를 DataFrame으로 저장\n",
    "generated_infer_results = pd.DataFrame(generated_infer_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_last_digit(s):\n",
    "    match = re.search(r'\\d$', s)  # 문자열 끝에서 숫자 하나만 매칭\n",
    "    return match.group() if match else None  # 매칭된 숫자를 반환, 없으면 None 반환\n",
    "\n",
    "result = generated_infer_results\n",
    "# 데이터프레임에 적용\n",
    "result['text'] = result['answer']\n",
    "result['answer'] = result['answer'].apply(lambda x: x.split('#')[-1])\n",
    "result['answer'] = result['answer'].apply(extract_last_digit)\n",
    "result['answer'] = result['answer'].fillna(1)\n",
    "result['answer'] = result['answer'].apply(lambda x: int(x))\n",
    "submission = result[['id', 'text', 'answer']]\n",
    "submission['answer'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. 서학은  조선 후기 유서( 類書)가  조선시기부터  편찬한 유서가  조선의  서학을  수용하였다.  서학은  조선 후기 유서의  편찬 방식에 따라  필요에  맞게 유서를 편찬하였다.  조선의 유서가  조선시기부터  편찬한 유서가  조선의  서학을  수용하였다.  조선의 유서가  조선시기부터  편찬한 유서가  조선의  서학을  수용하였다.  조선의 유서가  조선시기부터  편찬한 유서가 '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
